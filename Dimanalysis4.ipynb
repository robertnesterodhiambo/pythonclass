{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBJECTIVES\n",
    "\n",
    "\n",
    "*Background:*\n",
    "The company has collected a comprehensive set of sales data across various dimensions, including products, customers, regions, and time periods. The goal is to gain valuable insights into the sales performance and make data-driven decisions to optimize revenue and enhance overall business strategy.\n",
    "\n",
    "*Objectives:*\n",
    "\n",
    "1. **Sales Performance Overview:**\n",
    "   - Understand the historical sales trends and patterns to inform future business strategies.\n",
    "\n",
    "2. **Product Analysis:**\n",
    "   - Identify top-performing products and product categories.\n",
    "   - Explore opportunities for product portfolio enhancement and marketing strategies.\n",
    "\n",
    "3. **Customer Segmentation:**\n",
    "   - Segment customers based on their purchasing behavior and preferences.\n",
    "   - Tailor marketing and engagement strategies for different customer segments.\n",
    "\n",
    "4. **Geographical Analysis:**\n",
    "   - Evaluate sales performance across different regions.\n",
    "   - Identify regions with untapped potential or areas requiring special attention.\n",
    "\n",
    "5. **Time-based Analysis:**\n",
    "   - Analyze sales trends over different time dimensions (daily, monthly, yearly).\n",
    "   - Identify peak sales periods and strategize inventory and marketing efforts accordingly.\n",
    "\n",
    "6. **Promotion and Reseller Impact:**\n",
    "   - Evaluate the effectiveness of promotions on sales.\n",
    "   - Assess the contribution of different resellers to overall sales and optimize partnerships.\n",
    "\n",
    "7. **Financial Insights:**\n",
    "   - Calculate key financial metrics such as revenue, profit margins, and return on investment.\n",
    "   - Identify opportunities for cost optimization and revenue growth.\n",
    "\n",
    "*Outcome:*\n",
    "By addressing the objectives outlined above, the company aims to enhance its understanding of the sales landscape, identify growth opportunities, and make informed decisions to improve overall business performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data importation\n",
    "In setting up the data importation process, I began by initializing a Spark session, essentially creating a connection to the Spark framework for data processing. I specified the folder path where the CSV files are located, and I compiled a list of the CSV files I intended to import, including \"DimCurrency.csv,\" \"DimCustomer.csv,\" \"DimProduct.csv,\" \"DimGeography.csv,\" \"DimPromotion.csv,\" and \"DimSalesTerritory.csv.\"\n",
    "\n",
    "For each CSV file in the list, I used the `spark.read.csv` method to read the contents into a Spark DataFrame. The `header=True` parameter was set to indicate that the first row of each CSV file contains column names, and `inferSchema=True` was used to let Spark automatically infer the data types of the columns. After loading each DataFrame, I created a variable for it, naming it according to the base name of the corresponding CSV file without the file extension.\n",
    "\n",
    "This approach allows for convenient referencing of each DataFrame using variable names aligned with the file names. For instance, the `DimCurrency` DataFrame holds information about currencies, and `DimCustomer` contains details about customers. This naming convention facilitates subsequent analyses aligned with the objectives we established earlier, such as exploring product-centric data, customer segmentation, geographical analysis, and various time-based and financial insights. The groundwork has been laid for a comprehensive sales analysis, with the ability to delve into each aspect individually or combine dimensions for more complex analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------+\n",
      "|CurrencyKey|CurrencyAlternateKey|  CurrencyName|\n",
      "+-----------+--------------------+--------------+\n",
      "|          1|                 AFA|       Afghani|\n",
      "|          2|                 DZD|Algerian Dinar|\n",
      "|          3|                 ARS|Argentine Peso|\n",
      "|          4|                 AMD| Armenian Dram|\n",
      "|          5|                 AWG|Aruban Guilder|\n",
      "+-----------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+------------+--------------------+-----+---------+----------+--------+---------+---------+-------------+------+------+--------------------+------------+-------------+--------------------+----------------+----------------+---------------+-----------------+-----------------+----------------+--------------+---------------+-------------------+------------+-------------------+-----------------+---------------+\n",
      "|CustomerKey|GeographyKey|CustomerAlternateKey|Title|FirstName|MiddleName|LastName|NameStyle|BirthDate|MaritalStatus|Suffix|Gender|        EmailAddress|YearlyIncome|TotalChildren|NumberChildrenAtHome|EnglishEducation|SpanishEducation|FrenchEducation|EnglishOccupation|SpanishOccupation|FrenchOccupation|HouseOwnerFlag|NumberCarsOwned|       AddressLine1|AddressLine2|              Phone|DateFirstPurchase|CommuteDistance|\n",
      "+-----------+------------+--------------------+-----+---------+----------+--------+---------+---------+-------------+------+------+--------------------+------------+-------------+--------------------+----------------+----------------+---------------+-----------------+-----------------+----------------+--------------+---------------+-------------------+------------+-------------------+-----------------+---------------+\n",
      "|      11000|          26|          AW00011000| NULL|      Jon|         V|    Yang|    false| 4/8/1966|            M|  NULL|     M|jon24@adventure-w...|       90000|            2|                   0|       Bachelors|    Licenciatura|        Bac + 4|     Professional|      Profesional|           Cadre|             1|              0|    3761 N. 14th St|        NULL|1 (11) 500 555-0162|        7/22/2005|      1-2 Miles|\n",
      "|      11001|          37|          AW00011001| NULL|   Eugene|         L|   Huang|    false|5/14/1965|            S|  NULL|     M|eugene10@adventur...|       60000|            3|                   3|       Bachelors|    Licenciatura|        Bac + 4|     Professional|      Profesional|           Cadre|             0|              1|         2243 W St.|        NULL|1 (11) 500 555-0110|        7/18/2005|      0-1 Miles|\n",
      "|      11002|          31|          AW00011002| NULL|    Ruben|      NULL|  Torres|    false|8/12/1965|            M|  NULL|     M|ruben35@adventure...|       60000|            3|                   3|       Bachelors|    Licenciatura|        Bac + 4|     Professional|      Profesional|           Cadre|             1|              1|   5844 Linden Land|        NULL|1 (11) 500 555-0184|        7/10/2005|      2-5 Miles|\n",
      "|      11003|          11|          AW00011003| NULL|  Christy|      NULL|     Zhu|    false|2/15/1968|            S|  NULL|     F|christy12@adventu...|       70000|            0|                   0|       Bachelors|    Licenciatura|        Bac + 4|     Professional|      Profesional|           Cadre|             0|              1|   1825 Village Pl.|        NULL|1 (11) 500 555-0162|         7/1/2005|     5-10 Miles|\n",
      "|      11004|          19|          AW00011004| NULL|Elizabeth|      NULL| Johnson|    false| 8/8/1968|            S|  NULL|     F|elizabeth5@advent...|       80000|            5|                   5|       Bachelors|    Licenciatura|        Bac + 4|     Professional|      Profesional|           Cadre|             1|              4|7553 Harness Circle|        NULL|1 (11) 500 555-0131|        7/26/2005|      1-2 Miles|\n",
      "+-----------+------------+--------------------+-----+---------+----------+--------+---------+---------+-------------+------+------+--------------------+------------+-------------+--------------------+----------------+----------------+---------------+-----------------+-----------------+----------------+--------------+---------------+-------------------+------------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------------------+---------------------+---------------------+-------------------+--------------------+------------------+-----------------+------------+-----------------+-----+----------------+------------+---------+----+---------+------+-----------------+-----------+-----------+-----+-----+---------+------------------+----------------+-------+-------+\n",
      "|ProductKey|ProductAlternateKey|ProductSubcategoryKey|WeightUnitMeasureCode|SizeUnitMeasureCode|  EnglishProductName|SpanishProductName|FrenchProductName|StandardCost|FinishedGoodsFlag|Color|SafetyStockLevel|ReorderPoint|ListPrice|Size|SizeRange|Weight|DaysToManufacture|ProductLine|DealerPrice|Class|Style|ModelName|EnglishDescription|       StartDate|EndDate| Status|\n",
      "+----------+-------------------+---------------------+---------------------+-------------------+--------------------+------------------+-----------------+------------+-----------------+-----+----------------+------------+---------+----+---------+------+-----------------+-----------+-----------+-----+-----+---------+------------------+----------------+-------+-------+\n",
      "|         1|            AR-5381|                    0|                 NULL|               NULL|     Adjustable Race|              NULL|             NULL|        NULL|            false|   NA|            1000|         750|     NULL|NULL|       NA|  NULL|                0|       NULL|       NULL| NULL| NULL|     NULL|              NULL|01/06/1998 00:00|   NULL|Current|\n",
      "|         2|            BA-8327|                    0|                 NULL|               NULL|        Bearing Ball|              NULL|             NULL|        NULL|            false|   NA|            1000|         750|     NULL|NULL|       NA|  NULL|                0|       NULL|       NULL| NULL| NULL|     NULL|              NULL|01/06/1998 00:00|   NULL|Current|\n",
      "|         3|            BE-2349|                    0|                 NULL|               NULL|     BB Ball Bearing|              NULL|             NULL|        NULL|            false|   NA|             800|         600|     NULL|NULL|       NA|  NULL|                1|       NULL|       NULL| NULL| NULL|     NULL|              NULL|01/06/1998 00:00|   NULL|Current|\n",
      "|         4|            BE-2908|                    0|                 NULL|               NULL|Headset Ball Bear...|              NULL|             NULL|        NULL|            false|   NA|             800|         600|     NULL|NULL|       NA|  NULL|                0|       NULL|       NULL| NULL| NULL|     NULL|              NULL|01/06/1998 00:00|   NULL|Current|\n",
      "|         5|            BL-2036|                    0|                 NULL|               NULL|               Blade|              NULL|             NULL|        NULL|            false|   NA|             800|         600|     NULL|NULL|       NA|  NULL|                1|       NULL|       NULL| NULL| NULL|     NULL|              NULL|01/06/1998 00:00|   NULL|Current|\n",
      "+----------+-------------------+---------------------+---------------------+-------------------+--------------------+------------------+-----------------+------------+-----------------+-----+----------------+------------+---------+----+---------+------+-----------------+-----------+-----------+-----+-----+---------+------------------+----------------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+-------------+-----------------+-----------------+-----------------+------------------------+------------------------+-----------------------+----------+-----------------+----------------+\n",
      "|GeographyKey|         City|StateProvinceCode|StateProvinceName|CountryRegionCode|EnglishCountryRegionName|SpanishCountryRegionName|FrenchCountryRegionName|PostalCode|SalesTerritoryKey|IpAddressLocator|\n",
      "+------------+-------------+-----------------+-----------------+-----------------+------------------------+------------------------+-----------------------+----------+-----------------+----------------+\n",
      "|           1|   Alexandria|              NSW|  New South Wales|               AU|               Australia|               Australia|              Australie|      2015|                9|    198.51.100.2|\n",
      "|           2|Coffs Harbour|              NSW|  New South Wales|               AU|               Australia|               Australia|              Australie|      2450|                9|    198.51.100.3|\n",
      "|           3| Darlinghurst|              NSW|  New South Wales|               AU|               Australia|               Australia|              Australie|      2010|                9|    198.51.100.4|\n",
      "|           4|     Goulburn|              NSW|  New South Wales|               AU|               Australia|               Australia|              Australie|      2580|                9|    198.51.100.5|\n",
      "|           5|    Lane Cove|              NSW|  New South Wales|               AU|               Australia|               Australia|              Australie|      1597|                9|    198.51.100.6|\n",
      "+------------+-------------+-----------------+-----------------+-----------------+------------------------+------------------------+-----------------------+----------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+---------------------+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+------------------------+------------------------+-----------------------+-------------+---------------+------+------+\n",
      "|PromotionKey|PromotionAlternateKey|EnglishPromotionName|SpanishPromotionName| FrenchPromotionName|DiscountPct|EnglishPromotionType|SpanishPromotionType|FrenchPromotionType|EnglishPromotionCategory|SpanishPromotionCategory|FrenchPromotionCategory|    StartDate|        EndDate|MinQty|MaxQty|\n",
      "+------------+---------------------+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+------------------------+------------------------+-----------------------+-------------+---------------+------+------+\n",
      "|           1|                    1|         No Discount|       Sin descuento|       Aucune remise|        0.0|         No Discount|       Sin descuento|      Aucune remise|             No Discount|           Sin descuento|          Aucune remise|6/1/2005 0:00|12/31/2008 0:00|     0|  NULL|\n",
      "|           2|                    2|Volume Discount 1...|Descuento por vol...|Remise sur quanti...|       0.02|     Volume Discount|Descuento por vol...|Remise sur quantit�|                Reseller|            Distribuidor|              Revendeur|7/1/2005 0:00| 6/30/2008 0:00|    11|    14|\n",
      "|           3|                    3|Volume Discount 1...|Descuento por vol...|Remise sur quanti...|       0.05|     Volume Discount|Descuento por vol...|Remise sur quantit�|                Reseller|            Distribuidor|              Revendeur|7/1/2005 0:00| 6/30/2008 0:00|    15|    24|\n",
      "|           4|                    4|Volume Discount 2...|Descuento por vol...|Remise sur quanti...|        0.1|     Volume Discount|Descuento por vol...|Remise sur quantit�|                Reseller|            Distribuidor|              Revendeur|7/1/2005 0:00| 6/30/2008 0:00|    25|    40|\n",
      "|           5|                    5|Volume Discount 4...|Descuento por vol...|Remise sur quanti...|       0.15|     Volume Discount|Descuento por vol...|Remise sur quantit�|                Reseller|            Distribuidor|              Revendeur|7/1/2005 0:00| 6/30/2008 0:00|    41|    60|\n",
      "+------------+---------------------+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+------------------------+------------------------+-----------------------+-------------+---------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|SalesTerritoryKey|SalesTerritoryAlternateKey|SalesTerritoryRegion|SalesTerritoryCountry|SalesTerritoryGroup|\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|                1|                         1|           Northwest|        United States|      North America|\n",
      "|                2|                         2|           Northeast|        United States|      North America|\n",
      "|                3|                         3|             Central|        United States|      North America|\n",
      "|                4|                         4|           Southwest|        United States|      North America|\n",
      "|                5|                         5|           Southeast|        United States|      North America|\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = \"C:/Users/neste/OneDrive/Desktop/karanja/DataSet_final/DataSet_final\"\n",
    "\n",
    "# List of CSV files to load\n",
    "files_to_load = [\n",
    "    \"DimCurrency.csv\",\n",
    "    \"DimCustomer.csv\",\n",
    "    \"DimProduct.csv\",\n",
    "    \"DimGeography.csv\",\n",
    "    \"DimPromotion.csv\",\n",
    "    \"DimSalesTerritory.csv\",\n",
    "    \"FactInternetSales.csv\"\n",
    "]\n",
    "\n",
    "# Load each CSV file into a Spark DataFrame and assign a name corresponding to the file name\n",
    "for file in files_to_load:\n",
    "    file_path = f\"{folder_path}/{file}\"\n",
    "    dataframe_name = file.split('.')[0]  # Use the file name without extension as the DataFrame name\n",
    "    globals()[dataframe_name] = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first 5 rows of each DataFrame\n",
    "DimCurrency.show(5)\n",
    "DimCustomer.show(5)\n",
    "DimProduct.show(5)\n",
    "DimGeography.show(5)\n",
    "DimPromotion.show(5)\n",
    "DimSalesTerritory.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING\n",
    "\n",
    "In the data cleaning process, I carefully curated the dataset to enhance its quality and prepare it for meaningful analysis. This step is essential because raw data often contains inconsistencies, missing values, and irrelevant information that can impede accurate insights. To begin, I loaded the relevant datasets, including `FactInternetSales`, and joined them with dimension tables such as `DimCustomer`, `DimProduct`, `DimPromotion`, `DimCurrency`, `DimSalesTerritory`, and eventually `DimGeography`. This amalgamation of data provided a comprehensive view of sales transactions, customer details, product information, and contextual factors.\n",
    "\n",
    "As part of the cleaning process, I identified specific columns that were deemed unnecessary for the intended analysis. These columns, including keys and redundant identifiers, were systematically dropped from the dataset to streamline it for focused exploration. By excluding irrelevant information, the resulting DataFrame, named `data_clean`, now offers a more concise and targeted perspective on sales data. This curated dataset serves as the foundation for subsequent analyses, enabling more efficient queries, insightful visualizations, and a deeper understanding of sales performance.\n",
    "\n",
    "The cleaning process is pivotal in ensuring data accuracy, reliability, and relevance. It facilitates a smoother analytical workflow, allowing for a more efficient extraction of meaningful patterns and trends. Moreover, it enhances the interpretability of the data, making it easier to draw actionable conclusions and make informed business decisions. As I move forward with the analysis, the meticulously cleaned dataset, now named `data_clean`, will be instrumental in uncovering valuable insights related to product performance, customer behavior, geographical trends, and various other dimensions outlined in the initial objectives.\n",
    "\n",
    "The meticulous data cleaning process aligns seamlessly with the objectives outlined at the beginning of our analysis journey. By selectively removing extraneous columns and redundant information, the cleaned dataset, now designated as `data_clean`, is tailored to address specific facets of the sales analysis objectives. For instance, the removal of irrelevant columns streamlines the exploration of product-centric data, enabling a closer examination of top-selling products and their associated attributes.\n",
    "\n",
    "Furthermore, the cleaned dataset is instrumental in customer segmentation analysis. The exclusion of unnecessary customer details not only enhances data clarity but also focuses the analysis on key factors influencing customer behavior. Geographical analysis is also streamlined, as the cleaning process ensures that only pertinent location-related information remains in the dataset, facilitating a more precise examination of sales patterns across different regions.\n",
    "\n",
    "The data cleaning steps, performed with the objectives in mind, contribute to the overall effectiveness of subsequent analyses. The resulting `data_clean` dataset provides a solid foundation for exploring time-based trends, evaluating the impact of promotions, assessing reseller contributions, and calculating financial metrics. The alignment of the cleaning process with the objectives ensures that the subsequent analyses are not only accurate but also directly contribute to achieving the overarching goals of gaining actionable insights into sales performance and making informed business decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------+--------------------+--------------+-------------------+----------------+-----------+--------+-------+----------+-------------+------+------+------------+-------------+--------------------+----------------+-----------------+--------------+---------------+-------------------+---------------+-------------+------------------------+----------+\n",
      "|OrderDateKey|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|DiscountAmount|ProductStandardCost|TotalProductCost|SalesAmount|  TaxAmt|Freight| BirthDate|MaritalStatus|Suffix|Gender|YearlyIncome|TotalChildren|NumberChildrenAtHome|EnglishEducation|EnglishOccupation|HouseOwnerFlag|NumberCarsOwned|       AddressLine1|CommuteDistance|         City|EnglishCountryRegionName|PostalCode|\n",
      "+------------+---------+--------------+--------------------+--------------+-------------------+----------------+-----------+--------+-------+----------+-------------+------+------+------------+-------------+--------------------+----------------+-----------------+--------------+---------------+-------------------+---------------+-------------+------------------------+----------+\n",
      "|    20101229|  3578.27|       3578.27|                   0|             0|          2171.2942|       2171.2942|    3578.27|286.2616|89.4568| 8/22/1946|            S|  NULL|     M|       70000|            5|                   0|       Bachelors|       Management|             1|              3|   601 Asilomar Dr.|      10+ Miles|    Metchosin|                  Canada|        V9|\n",
      "|    20101229|  3399.99|       3399.99|                   0|             0|          1912.1544|       1912.1544|    3399.99|271.9992|84.9998|12/18/1964|            S|  NULL|     F|       20000|            3|                   3|     High School|           Manual|             0|              0| 14, avenue du Port|      0-1 Miles|       Pantin|                  France|     93500|\n",
      "|    20101229|  3399.99|       3399.99|                   0|             0|          1912.1544|       1912.1544|    3399.99|271.9992|84.9998| 12/3/1946|            S|  NULL|     F|       40000|            5|                   0|     High School|     Professional|             1|              3|4193 E. 28th Street|      10+ Miles|      Lebanon|           United States|     97355|\n",
      "|    20101229| 699.0982|      699.0982|                   0|             0|           413.1463|        413.1463|   699.0982| 55.9279|17.4775| 5/13/1938|            M|  NULL|     M|       80000|            4|                   0| Graduate Degree|       Management|             1|              2|  249 Alexander Pl.|      1-2 Miles|Beverly Hills|           United States|     90210|\n",
      "|    20101229|  3399.99|       3399.99|                   0|             0|          1912.1544|       1912.1544|    3399.99|271.9992|84.9998| 2/15/1968|            S|  NULL|     F|       70000|            0|                   0|       Bachelors|     Professional|             0|              1|   1825 Village Pl.|     5-10 Miles|   North Ryde|               Australia|      2113|\n",
      "+------------+---------+--------------+--------------------+--------------+-------------------+----------------+-----------+--------+-------+----------+-------------+------+------+------------+-------------+--------------------+----------------+-----------------+--------------+---------------+-------------------+---------------+-------------+------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Joining FactInternetSales with DimCustomer on the CustomerKey column\n",
    "joined_df = FactInternetSales.join(DimCustomer, FactInternetSales.CustomerKey == DimCustomer.CustomerKey, \"inner\")\n",
    "\n",
    "# Joining FactInternetSales with DimProduct on the ProductKey column\n",
    "joined_df = FactInternetSales.join(DimProduct, FactInternetSales.ProductKey == DimProduct.ProductKey, \"inner\")\n",
    "\n",
    "# Joining FactInternetSales with DimPromotion on PromotionKey column\n",
    "joined_df = FactInternetSales.join(DimPromotion, FactInternetSales.PromotionKey == DimPromotion.PromotionKey, \"inner\")\n",
    "\n",
    "# Joining with DimCurrency on CurrencyKey column\n",
    "joined_df = joined_df.join(DimCurrency, joined_df.CurrencyKey == DimCurrency.CurrencyKey, \"inner\")\n",
    "\n",
    "# Joining with DimSalesTerritory on SalesTerritoryKey column\n",
    "joined_df = joined_df.join(DimSalesTerritory, joined_df.SalesTerritoryKey == DimSalesTerritory.SalesTerritoryKey, \"inner\")\n",
    "\n",
    "\n",
    "# Joining FactInternetSales with DimCustomer on CustomerKey column\n",
    "joined_df = FactInternetSales.join(DimCustomer, FactInternetSales.CustomerKey == DimCustomer.CustomerKey, \"inner\")\n",
    "\n",
    "# Joining joined_df with DimGeography on GeographyKey column\n",
    "joined_df = joined_df.join(DimGeography, joined_df.GeographyKey == DimGeography.GeographyKey, \"inner\")\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = [\n",
    "    \"ProductKey\", \"DueDateKey\", \"ShipDateKey\", \"CustomerKey\", \"PromotionKey\", \n",
    "    \"CurrencyKey\", \"SalesTerritoryKey\", \"SalesOrderNumber\", \"SalesOrderLineNumber\", \n",
    "    \"RevisionNumber\", \"OrderQuantity\", \"CarrierTrackingNumber\", \"CustomerPONumber\", \n",
    "    \"OrderDate\", \"DueDate\", \"ShipDate\", \"CustomerKey\", \"GeographyKey\", \n",
    "    \"CustomerAlternateKey\", \"Title\", \"FirstName\", \"MiddleName\", \"LastName\", \n",
    "    \"NameStyle\", \"EmailAddress\", \"SpanishEducation\", \"FrenchEducation\", \n",
    "    \"SpanishOccupation\", \"FrenchOccupation\", \"AddressLine2\", \"Phone\", \n",
    "    \"DateFirstPurchase\", \"GeographyKey\", \"StateProvinceCode\", \n",
    "    \"StateProvinceName\", \"CountryRegionCode\", \"SpanishCountryRegionName\", \n",
    "    \"FrenchCountryRegionName\", \"SalesTerritoryKey\", \"IpAddressLocator\"\n",
    "]\n",
    "\n",
    "# Dropping the specified columns and naming the cleaned DataFrame as data_clean\n",
    "data_clean = joined_df.drop(*columns_to_drop)\n",
    "\n",
    "# Displaying the cleaned DataFrame\n",
    "data_clean.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date wrangling\n",
    "\n",
    "Navigating through the dataset's intricacies, I addressed the unconventional format of the `OrderDateKey` column, opting to convert it into a recognizable string format to facilitate subsequent date functions. Correcting the explicit type conversion, I successfully transformed the column, laying the groundwork for further temporal analysis.\n",
    "\n",
    "Building on this foundation, the creation of a new `OrderDate` column emerged, capturing the temporal aspect crucial for our problem statement's emphasis on understanding sales trends over time. This transformation set the stage for a nuanced exploration of temporal patterns, aligning precisely with the problem statement's call for a comprehensive analysis of sales data.\n",
    "\n",
    "Expanding the temporal granularity, I extracted the year, month, and day of the week from the `OrderDate` column. This intricate yet purposeful step allowed for a detailed examination of sales trends, ensuring a more profound understanding of temporal patterns in the dataset.\n",
    "\n",
    "In essence, the date wrangling process unfolded strategically, transforming unintuitive date representations into meaningful temporal information. This alignment with the problem statement ensures that subsequent analyses will yield insights directly relevant to the overarching objectives of understanding and optimizing sales performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------------+--------------+-------------------+----------------+-----------+--------+-------+-------------+------+------+------------+-------------+--------------------+----------------+-----------------+--------------+---------------+-------------------+---------------+-------------+------------------------+----------+---------+----------+--------------+----+\n",
      "|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|DiscountAmount|ProductStandardCost|TotalProductCost|SalesAmount|  TaxAmt|Freight|MaritalStatus|Suffix|Gender|YearlyIncome|TotalChildren|NumberChildrenAtHome|EnglishEducation|EnglishOccupation|HouseOwnerFlag|NumberCarsOwned|       AddressLine1|CommuteDistance|         City|EnglishCountryRegionName|PostalCode|OrderYear|OrderMonth|OrderDayOfWeek| Age|\n",
      "+---------+--------------+--------------------+--------------+-------------------+----------------+-----------+--------+-------+-------------+------+------+------------+-------------+--------------------+----------------+-----------------+--------------+---------------+-------------------+---------------+-------------+------------------------+----------+---------+----------+--------------+----+\n",
      "|  3578.27|       3578.27|                   0|             0|          2171.2942|       2171.2942|    3578.27|286.2616|89.4568|            S|  NULL|     M|       70000|            5|                   0|       Bachelors|       Management|             1|              3|   601 Asilomar Dr.|      10+ Miles|    Metchosin|                  Canada|        V9|     2010|        12|             4|77.0|\n",
      "|  3399.99|       3399.99|                   0|             0|          1912.1544|       1912.1544|    3399.99|271.9992|84.9998|            S|  NULL|     F|       20000|            3|                   3|     High School|           Manual|             0|              0| 14, avenue du Port|      0-1 Miles|       Pantin|                  France|     93500|     2010|        12|             4|59.0|\n",
      "|  3399.99|       3399.99|                   0|             0|          1912.1544|       1912.1544|    3399.99|271.9992|84.9998|            S|  NULL|     F|       40000|            5|                   0|     High School|     Professional|             1|              3|4193 E. 28th Street|      10+ Miles|      Lebanon|           United States|     97355|     2010|        12|             4|77.0|\n",
      "| 699.0982|      699.0982|                   0|             0|           413.1463|        413.1463|   699.0982| 55.9279|17.4775|            M|  NULL|     M|       80000|            4|                   0| Graduate Degree|       Management|             1|              2|  249 Alexander Pl.|      1-2 Miles|Beverly Hills|           United States|     90210|     2010|        12|             4|86.0|\n",
      "|  3399.99|       3399.99|                   0|             0|          1912.1544|       1912.1544|    3399.99|271.9992|84.9998|            S|  NULL|     F|       70000|            0|                   0|       Bachelors|     Professional|             0|              1|   1825 Village Pl.|     5-10 Miles|   North Ryde|               Australia|      2113|     2010|        12|             4|56.0|\n",
      "+---------+--------------+--------------------+--------------+-------------------+----------------+-----------+--------+-------+-------------+------+------+------------+-------------+--------------------+----------------+-----------------+--------------+---------------+-------------------+---------------+-------------+------------------------+----------+---------+----------+--------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming data_clean is the DataFrame containing the OrderDateKey column\n",
    "data_clean = data_clean.withColumn(\"OrderDateKey\", F.col(\"OrderDateKey\").cast(\"string\"))\n",
    "\n",
    "# Applying the date transformation\n",
    "data_clean = data_clean.withColumn(\n",
    "    \"OrderDate\",\n",
    "    F.from_unixtime(F.unix_timestamp(F.col(\"OrderDateKey\"), \"yyyyMMdd\")).cast(\"date\")\n",
    ")\n",
    "\n",
    "# Dropping the original OrderDateKey column\n",
    "data_clean = data_clean.drop(\"OrderDateKey\")\n",
    "\n",
    "\n",
    "# Assuming data_clean is the DataFrame containing the OrderDate column\n",
    "data_clean = data_clean.withColumn(\"OrderYear\", F.year(\"OrderDate\"))\n",
    "data_clean = data_clean.withColumn(\"OrderMonth\", F.month(\"OrderDate\"))\n",
    "data_clean = data_clean.withColumn(\"OrderDayOfWeek\", F.dayofweek(\"OrderDate\"))\n",
    "\n",
    "\n",
    "# Convert BirthDate to a valid date format\n",
    "data_clean = data_clean.withColumn(\"BirthDate\", F.to_date(\"BirthDate\", \"M/d/yyyy\"))\n",
    "\n",
    "# Drop rows with null values in BirthDate\n",
    "data_clean = data_clean.filter(F.col(\"BirthDate\").isNotNull())\n",
    "\n",
    "# Calculate age by subtracting BirthDate from the current date\n",
    "data_clean = data_clean.withColumn(\"Age\", F.datediff(F.current_date(), \"BirthDate\") / 365.25)\n",
    "\n",
    "# Drop the original BirthDate and OrderDate columns\n",
    "data_clean = data_clean.drop(\"BirthDate\", \"OrderDate\")\n",
    "\n",
    "# Round off the Age column to whole numbers\n",
    "data_clean = data_clean.withColumn(\"Age\", F.round(\"Age\"))\n",
    "\n",
    "# Displaying the updated DataFrame\n",
    "data_clean.show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding\n",
    "\n",
    "In the data encoding process, I utilized PySpark's StringIndexer to assign unique numerical indices to categorical columns, and then applied a custom mapping using dense_rank() to represent those indices as sequential integers. This encoding scheme ensures that each distinct string value in the categorical columns is associated with a specific integer, allowing for a numerical representation of categorical data.\n",
    "\n",
    "The rationale behind this encoding aligns with the analysis objectives, particularly in the context of machine learning or statistical analysis. Many machine learning algorithms require numerical input, and encoding categorical variables in this manner facilitates the application of these algorithms to the dataset. It simplifies the interpretation of categorical information by transforming it into a format that can be effectively processed and analyzed by various analytical models.\n",
    "\n",
    "By conforming to this encoding strategy, the data becomes more amenable to statistical analyses, predictive modeling, and other machine learning techniques that may be employed to gain insights into the sales data. The sequential integers assigned to unique values in the categorical columns maintain their order, preserving inherent relationships among categories. This structured representation of categorical data contributes to the effectiveness and interpretability of subsequent analyses, ultimately supporting the achievement of the predefined analysis objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------------+--------------+-------------------+----------------+-----------+--------+-------+------------+-------------+--------------------+--------------+---------------+---------+----------+--------------+----+---------------------+--------------+--------------+------------------------+-------------------------+--------------------+-----------------------+------------+--------------------------------+------------------+---------------------+--------------+--------------+------------------------+-------------------------+--------------------+-----------------------+------------+--------------------------------+------------------+\n",
      "|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|DiscountAmount|ProductStandardCost|TotalProductCost|SalesAmount|  TaxAmt|Freight|YearlyIncome|TotalChildren|NumberChildrenAtHome|HouseOwnerFlag|NumberCarsOwned|OrderYear|OrderMonth|OrderDayOfWeek| Age|MaritalStatus_encoded|Suffix_encoded|Gender_encoded|EnglishEducation_encoded|EnglishOccupation_encoded|AddressLine1_encoded|CommuteDistance_encoded|City_encoded|EnglishCountryRegionName_encoded|PostalCode_encoded|MaritalStatus_encoded|Suffix_encoded|Gender_encoded|EnglishEducation_encoded|EnglishOccupation_encoded|AddressLine1_encoded|CommuteDistance_encoded|City_encoded|EnglishCountryRegionName_encoded|PostalCode_encoded|\n",
      "+---------+--------------+--------------------+--------------+-------------------+----------------+-----------+--------+-------+------------+-------------+--------------------+--------------+---------------+---------+----------+--------------+----+---------------------+--------------+--------------+------------------------+-------------------------+--------------------+-----------------------+------------+--------------------------------+------------------+---------------------+--------------+--------------+------------------------+-------------------------+--------------------+-----------------------+------------+--------------------------------+------------------+\n",
      "|  2443.35|       2443.35|                   0|             0|          1518.7864|       1518.7864|    2443.35| 195.468|61.0838|       60000|            1|                   0|             0|              1|     2012|         6|             2|63.0|                  1.0|           1.0|           0.0|                     1.0|                      1.0|              1244.0|                    0.0|        34.0|                             1.0|              30.0|                    2|             1|             2|                       4|                        5|                 340|                      1|         242|                               1|                 1|\n",
      "|    29.99|         29.99|                   0|             0|            11.2163|         11.2163|      29.99|  2.3992| 0.7498|       80000|            5|                   5|             1|              4|     2013|        11|             3|55.0|                  0.0|           1.0|           0.0|                     0.0|                      0.0|              1256.0|                    0.0|        34.0|                             1.0|              30.0|                    1|             1|             2|                       1|                        4|                 545|                      1|         242|                               1|                 1|\n",
      "|  1700.99|       1700.99|                   0|             0|            1082.51|         1082.51|    1700.99|136.0792|42.5248|       60000|            1|                   0|             0|              1|     2013|        12|             2|63.0|                  1.0|           1.0|           0.0|                     1.0|                      1.0|              1244.0|                    0.0|        34.0|                             1.0|              30.0|                    2|             1|             2|                       4|                        5|                 340|                      1|         242|                               1|                 1|\n",
      "|     3.99|          3.99|                   0|             0|             1.4923|          1.4923|       3.99|  0.3192| 0.0998|       80000|            5|                   5|             1|              4|     2013|         4|             6|55.0|                  0.0|           1.0|           0.0|                     0.0|                      0.0|              1256.0|                    0.0|        34.0|                             1.0|              30.0|                    1|             1|             2|                       1|                        4|                 545|                      1|         242|                               1|                 1|\n",
      "|     32.6|          32.6|                   0|             0|            12.1924|         12.1924|       32.6|   2.608|  0.815|       80000|            5|                   5|             1|              4|     2013|         4|             6|55.0|                  0.0|           1.0|           0.0|                     0.0|                      0.0|              1256.0|                    0.0|        34.0|                             1.0|              30.0|                    1|             1|             2|                       1|                        4|                 545|                      1|         242|                               1|                 1|\n",
      "+---------+--------------+--------------------+--------------+-------------------+----------------+-----------+--------+-------+------------+-------------+--------------------+--------------+---------------+---------+----------+--------------+----+---------------------+--------------+--------------+------------------------+-------------------------+--------------------+-----------------------+------------+--------------------------------+------------------+---------------------+--------------+--------------+------------------------+-------------------------+--------------------+-----------------------+------------+--------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming data_clean is the DataFrame containing the specified categorical columns\n",
    "categorical_columns = [\"MaritalStatus\", \"Suffix\", \"Gender\", \"EnglishEducation\", \"EnglishOccupation\", \"AddressLine1\", \"CommuteDistance\", \"City\", \"EnglishCountryRegionName\", \"PostalCode\"]\n",
    "\n",
    "# Apply StringIndexer with handleInvalid=\"keep\" to obtain indexed values\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=f\"{column}_encoded\", handleInvalid=\"keep\") for column in categorical_columns]\n",
    "indexed_data = Pipeline(stages=indexers).fit(data_clean).transform(data_clean)\n",
    "\n",
    "# Create a mapping of unique values to sequential integers\n",
    "mapping_exprs = [f\"dense_rank() OVER (ORDER BY {col}) AS {col}_encoded\" for col in categorical_columns]\n",
    "\n",
    "# Apply the mapping expressions to replace indexed values with sequential integers\n",
    "data_encod = indexed_data.selectExpr(\"*\", *mapping_exprs)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "data_encod = data_encod.drop(*categorical_columns)\n",
    "\n",
    "# Displaying the updated DataFrame\n",
    "data_encod.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Assuming data_clean is a PySpark DataFrame\n",
    "# Convert it to a Pandas DataFrame\n",
    "data_clean_pd = data_clean.toPandas()\n",
    "\n",
    "# Replace 'OrderYear', 'OrderMonth', 'OrderDayOfWeek' with the actual column names in your DataFrame\n",
    "order_year_counts = data_clean_pd['OrderYear'].value_counts()\n",
    "order_month_counts = data_clean_pd['OrderMonth'].value_counts()\n",
    "order_day_of_week_counts = data_clean_pd['OrderDayOfWeek'].value_counts()\n",
    "\n",
    "# Create a pie chart for OrderYear\n",
    "fig1 = px.pie(order_year_counts, names=order_year_counts.index, title='Order Year')\n",
    "\n",
    "# Create a pie chart for OrderMonth\n",
    "fig2 = px.pie(order_month_counts, names=order_month_counts.index, title='Order Month')\n",
    "\n",
    "# Create a pie chart for OrderDayOfWeek\n",
    "fig3 = px.pie(order_day_of_week_counts, names=order_day_of_week_counts.index, title='Order Day of Week')\n",
    "\n",
    "# Display the charts\n",
    "fig1.show()\n",
    "fig2.show()\n",
    "fig3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
