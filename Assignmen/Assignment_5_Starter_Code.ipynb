{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Libraries"
      ],
      "metadata": {
        "id": "jSc5t0lJkApI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNbFHPSxYfD0"
      },
      "outputs": [],
      "source": [
        "!pip install ucimlrepo --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo"
      ],
      "metadata": {
        "id": "C-RbMUy4Yovj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Random Seed\n",
        "\n",
        "Make sure that you use this on every single call to the `train_test_split` function"
      ],
      "metadata": {
        "id": "0-kPpX0dkEP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 123456789"
      ],
      "metadata": {
        "id": "PwV5ToFsYqyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch and One-Hot Encode Datasets"
      ],
      "metadata": {
        "id": "iAX1hfnIkLyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch dataset\n",
        "bank_marketing = fetch_ucirepo(id=222)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "bank_marketing_features = bank_marketing.data.features\n",
        "bank_marketing_features_onehot = pd.get_dummies(bank_marketing_features)\n",
        "bank_marketing_labels = bank_marketing.data.targets\n",
        "\n",
        "# metadata\n",
        "print(bank_marketing.metadata)\n",
        "\n",
        "# variable information\n",
        "print(bank_marketing.variables)"
      ],
      "metadata": {
        "id": "taeJ9LYijqs2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a2d407-21ef-41f5-9ea4-437392b0e62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 222, 'name': 'Bank Marketing', 'repository_url': 'https://archive.ics.uci.edu/dataset/222/bank+marketing', 'data_url': 'https://archive.ics.uci.edu/static/public/222/data.csv', 'abstract': 'The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).', 'area': 'Business', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 45211, 'num_features': 16, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Occupation', 'Marital Status', 'Education Level'], 'target_col': ['y'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 2014, 'last_updated': 'Fri Aug 18 2023', 'dataset_doi': '10.24432/C5K306', 'creators': ['S. Moro', 'P. Rita', 'P. Cortez'], 'intro_paper': {'ID': 277, 'type': 'NATIVE', 'title': 'A data-driven approach to predict the success of bank telemarketing', 'authors': 'Sérgio Moro, P. Cortez, P. Rita', 'venue': 'Decision Support Systems', 'year': 2014, 'journal': None, 'DOI': '10.1016/j.dss.2014.03.001', 'URL': 'https://www.semanticscholar.org/paper/cab86052882d126d43f72108c6cb41b295cc8a9e', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': \"The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \\n\\nThere are four datasets: \\n1) bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\\n2) bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs.\\n3) bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs). \\n4) bank.csv with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs). \\nThe smallest datasets are provided to test more computationally demanding machine learning algorithms (e.g., SVM). \\n\\nThe classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Input variables:\\n   # bank client data:\\n   1 - age (numeric)\\n   2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\\n                                       \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\") \\n   3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\\n   4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\\n   5 - default: has credit in default? (binary: \"yes\",\"no\")\\n   6 - balance: average yearly balance, in euros (numeric) \\n   7 - housing: has housing loan? (binary: \"yes\",\"no\")\\n   8 - loan: has personal loan? (binary: \"yes\",\"no\")\\n   # related with the last contact of the current campaign:\\n   9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\") \\n  10 - day: last contact day of the month (numeric)\\n  11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\\n  12 - duration: last contact duration, in seconds (numeric)\\n   # other attributes:\\n  13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\\n  14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\\n  15 - previous: number of contacts performed before this campaign and for this client (numeric)\\n  16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\\n\\n  Output variable (desired target):\\n  17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\\n', 'citation': None}}\n",
            "           name     role         type      demographic  \\\n",
            "0           age  Feature      Integer              Age   \n",
            "1           job  Feature  Categorical       Occupation   \n",
            "2       marital  Feature  Categorical   Marital Status   \n",
            "3     education  Feature  Categorical  Education Level   \n",
            "4       default  Feature       Binary             None   \n",
            "5       balance  Feature      Integer             None   \n",
            "6       housing  Feature       Binary             None   \n",
            "7          loan  Feature       Binary             None   \n",
            "8       contact  Feature  Categorical             None   \n",
            "9   day_of_week  Feature         Date             None   \n",
            "10        month  Feature         Date             None   \n",
            "11     duration  Feature      Integer             None   \n",
            "12     campaign  Feature      Integer             None   \n",
            "13        pdays  Feature      Integer             None   \n",
            "14     previous  Feature      Integer             None   \n",
            "15     poutcome  Feature  Categorical             None   \n",
            "16            y   Target       Binary             None   \n",
            "\n",
            "                                          description  units missing_values  \n",
            "0                                                None   None             no  \n",
            "1   type of job (categorical: 'admin.','blue-colla...   None             no  \n",
            "2   marital status (categorical: 'divorced','marri...   None             no  \n",
            "3   (categorical: 'basic.4y','basic.6y','basic.9y'...   None             no  \n",
            "4                              has credit in default?   None             no  \n",
            "5                              average yearly balance  euros             no  \n",
            "6                                   has housing loan?   None             no  \n",
            "7                                  has personal loan?   None             no  \n",
            "8   contact communication type (categorical: 'cell...   None            yes  \n",
            "9                        last contact day of the week   None             no  \n",
            "10  last contact month of year (categorical: 'jan'...   None             no  \n",
            "11   last contact duration, in seconds (numeric). ...   None             no  \n",
            "12  number of contacts performed during this campa...   None             no  \n",
            "13  number of days that passed by after the client...   None            yes  \n",
            "14  number of contacts performed before this campa...   None             no  \n",
            "15  outcome of the previous marketing campaign (ca...   None            yes  \n",
            "16          has the client subscribed a term deposit?   None             no  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cc_default = fetch_ucirepo(id=350)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "cc_default_features = cc_default.data.features\n",
        "cc_default_features_onehot = pd.get_dummies(cc_default_features, columns=['X2', 'X3', 'X4', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11'])\n",
        "cc_default_labels = cc_default.data.targets\n",
        "\n",
        "# metadata\n",
        "print(cc_default.metadata)\n",
        "\n",
        "# variable information\n",
        "print(cc_default.variables)"
      ],
      "metadata": {
        "id": "D_FmFq5pjtEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5081b9c9-5dc2-45c1-fba7-5951d13d7a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 350, 'name': 'Default of Credit Card Clients', 'repository_url': 'https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients', 'data_url': 'https://archive.ics.uci.edu/static/public/350/data.csv', 'abstract': \"This research aimed at the case of customers' default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods.\", 'area': 'Business', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 30000, 'num_features': 23, 'feature_types': ['Integer', 'Real'], 'demographics': ['Sex', 'Education Level', 'Marital Status', 'Age'], 'target_col': ['Y'], 'index_col': ['ID'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2009, 'last_updated': 'Fri Mar 29 2024', 'dataset_doi': '10.24432/C55S3H', 'creators': ['I-Cheng Yeh'], 'intro_paper': {'ID': 365, 'type': 'NATIVE', 'title': 'The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients', 'authors': 'I. Yeh, Che-hui Lien', 'venue': 'Expert systems with applications', 'year': 2009, 'journal': None, 'DOI': '10.1016/j.eswa.2007.12.020', 'URL': 'https://www.semanticscholar.org/paper/1cacac4f0ea9fdff3cd88c151c94115a9fddcf33', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': \"This research aimed at the case of customers' default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel Sorting Smoothing Method to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y = A + BX) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default.\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables:\\r\\nX1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\\r\\nX2: Gender (1 = male; 2 = female).\\r\\nX3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\\r\\nX4: Marital status (1 = married; 2 = single; 3 = others).\\r\\nX5: Age (year).\\r\\nX6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\\r\\nX12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005. \\r\\nX18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005.\\r\\n', 'citation': None}}\n",
            "   name     role     type      demographic                 description units  \\\n",
            "0    ID       ID  Integer             None                        None  None   \n",
            "1    X1  Feature  Integer             None                   LIMIT_BAL  None   \n",
            "2    X2  Feature  Integer              Sex                         SEX  None   \n",
            "3    X3  Feature  Integer  Education Level                   EDUCATION  None   \n",
            "4    X4  Feature  Integer   Marital Status                    MARRIAGE  None   \n",
            "5    X5  Feature  Integer              Age                         AGE  None   \n",
            "6    X6  Feature  Integer             None                       PAY_0  None   \n",
            "7    X7  Feature  Integer             None                       PAY_2  None   \n",
            "8    X8  Feature  Integer             None                       PAY_3  None   \n",
            "9    X9  Feature  Integer             None                       PAY_4  None   \n",
            "10  X10  Feature  Integer             None                       PAY_5  None   \n",
            "11  X11  Feature  Integer             None                       PAY_6  None   \n",
            "12  X12  Feature  Integer             None                   BILL_AMT1  None   \n",
            "13  X13  Feature  Integer             None                   BILL_AMT2  None   \n",
            "14  X14  Feature  Integer             None                   BILL_AMT3  None   \n",
            "15  X15  Feature  Integer             None                   BILL_AMT4  None   \n",
            "16  X16  Feature  Integer             None                   BILL_AMT5  None   \n",
            "17  X17  Feature  Integer             None                   BILL_AMT6  None   \n",
            "18  X18  Feature  Integer             None                    PAY_AMT1  None   \n",
            "19  X19  Feature  Integer             None                    PAY_AMT2  None   \n",
            "20  X20  Feature  Integer             None                    PAY_AMT3  None   \n",
            "21  X21  Feature  Integer             None                    PAY_AMT4  None   \n",
            "22  X22  Feature  Integer             None                    PAY_AMT5  None   \n",
            "23  X23  Feature  Integer             None                    PAY_AMT6  None   \n",
            "24    Y   Target   Binary             None  default payment next month  None   \n",
            "\n",
            "   missing_values  \n",
            "0              no  \n",
            "1              no  \n",
            "2              no  \n",
            "3              no  \n",
            "4              no  \n",
            "5              no  \n",
            "6              no  \n",
            "7              no  \n",
            "8              no  \n",
            "9              no  \n",
            "10             no  \n",
            "11             no  \n",
            "12             no  \n",
            "13             no  \n",
            "14             no  \n",
            "15             no  \n",
            "16             no  \n",
            "17             no  \n",
            "18             no  \n",
            "19             no  \n",
            "20             no  \n",
            "21             no  \n",
            "22             no  \n",
            "23             no  \n",
            "24             no  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1"
      ],
      "metadata": {
        "id": "BgqFXAstkRXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct Default Decision Tree for Bank Marketing"
      ],
      "metadata": {
        "id": "QuZRJkgekXaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the code below to construct, train, and evaluate the default decision tree on the bank marketing dataset\n",
        "# Make sure you report accuracy on the training set as well!\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data into training and testing sets (80-20 split)\n",
        "bank_marketing_features_train, bank_marketing_features_test, bank_marketing_labels_train, bank_marketing_labels_test = train_test_split(\n",
        "    bank_marketing_features_onehot,  # Preprocessed features\n",
        "    bank_marketing_labels,          # Target labels\n",
        "    test_size=0.2,                  # 20% for testing\n",
        "    random_state=42                 # Reproducibility\n",
        ")\n",
        "\n",
        "# Initialize the Decision Tree Classifier with default parameters\n",
        "bank_marketing_dt_default = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the decision tree on the training data\n",
        "bank_marketing_dt_default.fit(bank_marketing_features_train, bank_marketing_labels_train)\n",
        "\n",
        "# Make predictions on both the training and testing sets\n",
        "bank_marketing_train_predictions = bank_marketing_dt_default.predict(bank_marketing_features_train)\n",
        "bank_marketing_test_predictions = bank_marketing_dt_default.predict(bank_marketing_features_test)\n",
        "\n",
        "# Calculate accuracy for both training and testing sets\n",
        "train_accuracy = accuracy_score(bank_marketing_labels_train, bank_marketing_train_predictions)\n",
        "test_accuracy = accuracy_score(bank_marketing_labels_test, bank_marketing_test_predictions)\n",
        "\n",
        "# Output the training and testing accuracy\n",
        "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Testing Accuracy: {test_accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "rUKq6iF2j2_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e44de43-7674-4342-9aed-a8fed81c11e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 100.00%\n",
            "Testing Accuracy: 87.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2"
      ],
      "metadata": {
        "id": "s0sgUZD2mCxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data into training and testing sets (80-20 split)\n",
        "bank_marketing_features_train, bank_marketing_features_test, bank_marketing_labels_train, bank_marketing_labels_test = train_test_split(\n",
        "    bank_marketing_features_onehot,  # Preprocessed features\n",
        "    bank_marketing_labels,          # Target labels\n",
        "    test_size=0.2,                  # 20% for testing\n",
        "    random_state=42                 # Reproducibility\n",
        ")\n",
        "\n",
        "# Maximum depths to limit to\n",
        "max_depths = [1, 2, 3, 5, 7, 10, 15, 20]\n",
        "\n",
        "# Initialize a list to store results\n",
        "results = []\n",
        "\n",
        "# Loop through different maximum depths\n",
        "for depth in max_depths:\n",
        "    # Initialize a decision tree classifier with the specified max depth\n",
        "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    dt.fit(bank_marketing_features_train, bank_marketing_labels_train)\n",
        "\n",
        "    # Make predictions\n",
        "    train_predictions = dt.predict(bank_marketing_features_train)\n",
        "    test_predictions = dt.predict(bank_marketing_features_test)\n",
        "\n",
        "    # Calculate accuracies\n",
        "    train_accuracy = accuracy_score(bank_marketing_labels_train, train_predictions)\n",
        "    test_accuracy = accuracy_score(bank_marketing_labels_test, test_predictions)\n",
        "\n",
        "    # Append results\n",
        "    results.append({\"Depth\": depth, \"Training Accuracy\": train_accuracy, \"Testing Accuracy\": test_accuracy})\n",
        "\n",
        "# Convert results to a DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Print the table\n",
        "print(results_df)\n",
        "\n",
        "# Optional: Save the table to a CSV\n",
        "results_df.to_csv(\"decision_tree_depth_analysis.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "-IPJcPcjmGEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5509e769-9158-4f05-d5a6-56c0533c31a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Depth  Training Accuracy  Testing Accuracy\n",
            "0      1           0.883931          0.879354\n",
            "1      2           0.896925          0.893177\n",
            "2      3           0.902372          0.896495\n",
            "3      5           0.906741          0.897158\n",
            "4      7           0.912077          0.897822\n",
            "5     10           0.925265          0.897269\n",
            "6     15           0.958665          0.894062\n",
            "7     20           0.985291          0.884552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finish Q2 like Q1, but limiting maximum depth"
      ],
      "metadata": {
        "id": "m6inNTWpvOEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the dataset (reuse for consistency)\n",
        "bank_marketing_features_train, bank_marketing_features_test, bank_marketing_labels_train, bank_marketing_labels_test = train_test_split(\n",
        "    bank_marketing_features_onehot,  # Features with one-hot encoding\n",
        "    bank_marketing_labels,          # Target labels\n",
        "    test_size=0.2,                  # 80% train, 20% test split\n",
        "    random_state=42                 # Ensure reproducibility\n",
        ")\n",
        "\n",
        "# Maximum depths to evaluate\n",
        "max_depths = [1, 2, 3, 5, 7, 10, 15, 20]\n",
        "\n",
        "# Results storage\n",
        "results = []\n",
        "\n",
        "# Loop through each depth\n",
        "for depth in max_depths:\n",
        "    # Create and train a decision tree with the specified max depth\n",
        "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    dt.fit(bank_marketing_features_train, bank_marketing_labels_train)\n",
        "\n",
        "    # Evaluate performance\n",
        "    train_accuracy = accuracy_score(bank_marketing_labels_train, dt.predict(bank_marketing_features_train))\n",
        "    test_accuracy = accuracy_score(bank_marketing_labels_test, dt.predict(bank_marketing_features_test))\n",
        "\n",
        "    # Store the depth and accuracies\n",
        "    results.append({\"Depth\": depth, \"Training Accuracy\": train_accuracy, \"Testing Accuracy\": test_accuracy})\n",
        "\n",
        "# Convert results into a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Print results table\n",
        "print(results_df)\n",
        "\n",
        "# Save results as CSV for further use (optional)\n",
        "results_df.to_csv(\"decision_tree_max_depth_analysis.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "cr3sp9sNvOWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59db7f9-f3e4-4ddf-da47-dfc1ae203ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Depth  Training Accuracy  Testing Accuracy\n",
            "0      1           0.883931          0.879354\n",
            "1      2           0.896925          0.893177\n",
            "2      3           0.902372          0.896495\n",
            "3      5           0.906741          0.897158\n",
            "4      7           0.912077          0.897822\n",
            "5     10           0.925265          0.897269\n",
            "6     15           0.958665          0.894062\n",
            "7     20           0.985291          0.884552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3"
      ],
      "metadata": {
        "id": "by7_Sg0FnF8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct Default Decision Tree for Credit Card Default Dataset"
      ],
      "metadata": {
        "id": "YX5tC_kPnKlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data into training and testing sets (80-20 split)\n",
        "cc_default_features_train, cc_default_features_test, cc_default_labels_train, cc_default_labels_test = train_test_split(\n",
        "    cc_default_features_onehot,  # Preprocessed features with one-hot encoding\n",
        "    cc_default_labels,          # Target labels\n",
        "    test_size=0.2,              # 20% for testing\n",
        "    random_state=42             # Reproducibility\n",
        ")\n",
        "\n",
        "# Create a Decision Tree Classifier with default parameters\n",
        "dt_default = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model on the training set\n",
        "dt_default.fit(cc_default_features_train, cc_default_labels_train)\n",
        "\n",
        "# Make predictions on both the training and test sets\n",
        "train_predictions = dt_default.predict(cc_default_features_train)\n",
        "test_predictions = dt_default.predict(cc_default_features_test)\n",
        "\n",
        "# Calculate accuracy for training and test sets\n",
        "train_accuracy = accuracy_score(cc_default_labels_train, train_predictions)\n",
        "test_accuracy = accuracy_score(cc_default_labels_test, test_predictions)\n",
        "\n",
        "# Report accuracies\n",
        "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "uoaPHAlZnFIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80262a5-ddd7-49aa-ba5e-a81b98d74dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 99.95%\n",
            "Testing Accuracy: 72.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4"
      ],
      "metadata": {
        "id": "Is82noLpnRiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data into training and testing sets (80-20 split)\n",
        "cc_default_features_train, cc_default_features_test, cc_default_labels_train, cc_default_labels_test = train_test_split(\n",
        "    cc_default_features_onehot,  # Preprocessed features with one-hot encoding\n",
        "    cc_default_labels,          # Target labels\n",
        "    test_size=0.2,              # 20% for testing\n",
        "    random_state=42             # Reproducibility\n",
        ")\n",
        "\n",
        "# List of maximum depths to evaluate\n",
        "max_depths = [1, 2, 3, 5, 7, 10, 15, 20]\n",
        "\n",
        "# Initialize a list to store results\n",
        "results = []\n",
        "\n",
        "# Loop through each depth\n",
        "for depth in max_depths:\n",
        "    # Create a Decision Tree Classifier with the current max depth\n",
        "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "\n",
        "    # Train the model on the training set\n",
        "    dt.fit(cc_default_features_train, cc_default_labels_train)\n",
        "\n",
        "    # Make predictions on the training and test sets\n",
        "    train_predictions = dt.predict(cc_default_features_train)\n",
        "    test_predictions = dt.predict(cc_default_features_test)\n",
        "\n",
        "    # Calculate accuracy for training and test sets\n",
        "    train_accuracy = accuracy_score(cc_default_labels_train, train_predictions)\n",
        "    test_accuracy = accuracy_score(cc_default_labels_test, test_predictions)\n",
        "\n",
        "    # Append the results to the list\n",
        "    results.append({\n",
        "        \"Depth\": depth,\n",
        "        \"Training Accuracy\": train_accuracy,\n",
        "        \"Testing Accuracy\": test_accuracy\n",
        "    })\n",
        "\n",
        "# Convert results into a DataFrame for better readability\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Print results table\n",
        "print(results_df)\n",
        "\n",
        "# Save results as CSV for further use (optional)\n",
        "results_df.to_csv(\"decision_tree_depth_analysis.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "-2oOT9vQvl7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6852c8af-72ec-4b26-bb93-0eb47934ffe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Depth  Training Accuracy  Testing Accuracy\n",
            "0      1           0.812125          0.815667\n",
            "1      2           0.812958          0.816333\n",
            "2      3           0.818750          0.818833\n",
            "3      5           0.823625          0.820500\n",
            "4      7           0.829375          0.820000\n",
            "5     10           0.848042          0.809500\n",
            "6     15           0.892667          0.795667\n",
            "7     20           0.937417          0.768667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finish Q4 like Q3, but limiting maximum depth"
      ],
      "metadata": {
        "id": "0USwc6aqnS0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5"
      ],
      "metadata": {
        "id": "zRzbI0sRmdFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarities Between Logistic Regression and Classification Tree\n",
        "\n",
        "Both are supervised learning MachineLearning algorithms\n",
        "\n",
        "### Differences Between Logistic Regression and Classification Tree\n",
        "\n",
        "1. **Model Type:**\n",
        "   - **Logistic Regression**: Assume a linear model\n",
        "   - **Classification Tree**: Non-linear model that splits data based on feature values.\n",
        "\n",
        "2. **Interpretability:**\n",
        "   - **Logistic Regression**: Coefficients represent feature influence, but harder to visualize.\n",
        "   - **Classification Tree**: Easy to interpret with a tree structure, showing decision-making steps.\n",
        "\n",
        "3. **Assumptions:**\n",
        "   - **Logistic Regression**: Assumes linearity and independent errors.\n",
        "   - **Classification Tree**: Makes no assumptions about data distribution.\n",
        "\n",
        "4. **Feature Handling:**\n",
        "   - **Logistic Regression**: Works best with numerical features (needs encoding for categorical).\n",
        "   - **Classification Tree**: Handles both numerical and categorical features directly.\n",
        "\n",
        "5. **Overfitting:**\n",
        "   - **Logistic Regression**: Less prone to overfitting if regularized.\n",
        "   - **Classification Tree**: More prone to overfitting, especially without pruning.\n",
        "\n",
        "### When Would They Perform Similarly?\n",
        "\n",
        "- **Scenario:** **Linearly Separable Data** (e.g., a clear boundary separating classes in feature space).\n",
        "  - Both algorithms would perform similarly as logistic regression would fit a linear decision boundary and classification trees would make clear splits.\n",
        "\n",
        "### When Would They Perform Differently?\n",
        "\n",
        "- **Scenario:** **Non-Linearly Separable Data** (e.g., classes are intertwined or have complex patterns).\n",
        "  - **Logistic Regression**: Struggles with non-linear separability and performs poorly.\n",
        "  - **Classification Tree**: Performs well by making complex splits to capture non-linear relationships.\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **Similar Performance:** When the data is linearly separable.\n",
        "- **Different Performance:** In complex, non-linear datasets, classification trees tend to outperform logistic regression due to their flexibility."
      ],
      "metadata": {
        "id": "PGbr_F0Il0_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6"
      ],
      "metadata": {
        "id": "qR3lVhedmgVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I noticed that my coworker's decision tree is overfitting because the depth is set too high (10). With a small dataset (~500 data points), a deep tree becomes too complex and memorizes specific details and noise from the training data. This results in near-perfect training accuracy (~100%) but poor testing accuracy (~60%) because the model struggles to generalize. In small datasets, the model becomes overly specific to the training set, capturing noise instead of general patterns. To fix this, I would recommend reducing the tree’s depth or using pruning techniques to prevent overfitting and improve generalization."
      ],
      "metadata": {
        "id": "kRTvTcaxmia5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7"
      ],
      "metadata": {
        "id": "PnqqRF5Lm8er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, the logistic regression and decision tree do not match. Logistic regression uses a continuous, linear relationship between the features (weight, height, and age) and retention, as shown by the equation. In contrast, the decision tree creates discrete splits based on thresholds (e.g., weight > 200 or age > 40). This results in a non-linear, piecewise decision-making process. While both predict retention, the logistic regression model gives a smooth, continuous prediction, whereas the decision tree makes predictions based on specific feature thresholds."
      ],
      "metadata": {
        "id": "LskoX64lnIQX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pKR1rVHpmhta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}