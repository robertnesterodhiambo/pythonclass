{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening link 1\n",
      "Opening link 2\n",
      "Opening link 3\n",
      "Opening link 4\n",
      "Opening link 5\n",
      "Opening link 6\n",
      "Opening link 7\n",
      "Opening link 8\n",
      "Opening link 9\n",
      "Opening link 10\n",
      "Opening link 11\n",
      "Opening link 12\n",
      "Opening link 13\n",
      "Opening link 14\n",
      "Opening link 15\n",
      "Opening link 16\n",
      "Opening link 17\n",
      "Opening link 18\n",
      "Opening link 19\n",
      "Opening link 20\n",
      "Opening link 21\n",
      "Opening link 22\n",
      "Opening link 23\n",
      "Opening link 24\n",
      "Opening link 25\n",
      "Opening link 26\n",
      "Opening link 27\n",
      "Opening link 28\n",
      "Opening link 29\n",
      "Opening link 30\n",
      "Opening link 31\n",
      "Opening link 32\n",
      "Opening link 33\n",
      "Opening link 34\n",
      "Opening link 35\n",
      "Opening link 36\n",
      "Opening link 37\n",
      "Opening link 38\n",
      "Opening link 39\n",
      "Opening link 40\n",
      "Opening link 41\n",
      "Opening link 42\n",
      "Opening link 43\n",
      "Opening link 44\n",
      "Opening link 45\n",
      "Opening link 46\n",
      "Opening link 47\n",
      "Opening link 48\n",
      "Opening link 49\n",
      "Opening link 50\n",
      "Opening link 51\n",
      "Opening link 52\n",
      "Opening link 53\n",
      "Opening link 54\n",
      "Opening link 55\n",
      "Opening link 56\n",
      "Opening link 57\n",
      "Opening link 58\n",
      "Opening link 59\n",
      "Opening link 60\n",
      "Opening link 61\n",
      "Opening link 62\n",
      "Opening link 63\n",
      "Opening link 64\n",
      "Opening link 65\n",
      "Opening link 66\n",
      "Opening link 67\n",
      "Opening link 68\n",
      "Opening link 69\n",
      "Opening link 70\n",
      "Opening link 71\n",
      "Opening link 72\n",
      "Opening link 73\n",
      "Opening link 74\n",
      "Opening link 75\n",
      "Opening link 76\n",
      "Opening link 77\n",
      "Opening link 78\n",
      "Opening link 79\n",
      "Opening link 80\n",
      "Opening link 81\n",
      "Opening link 82\n",
      "Opening link 83\n",
      "Opening link 84\n",
      "Opening link 85\n",
      "Opening link 86\n",
      "Opening link 87\n",
      "Opening link 88\n",
      "Opening link 89\n",
      "Opening link 90\n",
      "Opening link 91\n",
      "Opening link 92\n",
      "Opening link 93\n",
      "Opening link 94\n",
      "Opening link 95\n",
      "Opening link 96\n",
      "Opening link 97\n",
      "Opening link 98\n",
      "Opening link 99\n",
      "Opening link 100\n",
      "Opening link 101\n",
      "Opening link 102\n",
      "Opening link 103\n",
      "Opening link 104\n",
      "Opening link 105\n",
      "Opening link 106\n",
      "Opening link 107\n",
      "Opening link 108\n",
      "Opening link 109\n",
      "Opening link 110\n",
      "Opening link 111\n",
      "Opening link 112\n",
      "Opening link 113\n",
      "Opening link 114\n",
      "Opening link 115\n",
      "Opening link 116\n",
      "Opening link 117\n",
      "Opening link 118\n",
      "Opening link 119\n",
      "Opening link 120\n",
      "Opening link 121\n",
      "Opening link 122\n",
      "Opening link 123\n",
      "Opening link 124\n",
      "Opening link 125\n",
      "Opening link 126\n",
      "Opening link 127\n",
      "Opening link 128\n",
      "Opening link 129\n",
      "Opening link 130\n",
      "Opening link 131\n",
      "Opening link 132\n",
      "Opening link 133\n",
      "Opening link 134\n",
      "Opening link 135\n",
      "Opening link 136\n",
      "Opening link 137\n",
      "Opening link 138\n",
      "Opening link 139\n",
      "Opening link 140\n",
      "Opening link 141\n",
      "Opening link 142\n",
      "Opening link 143\n",
      "Opening link 144\n",
      "Opening link 145\n",
      "Opening link 146\n",
      "Opening link 147\n",
      "Opening link 148\n",
      "Opening link 149\n",
      "Opening link 150\n",
      "Opening link 151\n",
      "Opening link 152\n",
      "Opening link 153\n",
      "Opening link 154\n",
      "Error processing link 154: Message: Unable to locate element: .affiliation; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:187:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:505:5\n",
      "element.find/</<@chrome://remote/content/marionette/element.sys.mjs:135:16\n",
      "\n",
      "Opening link 155\n",
      "Opening link 156\n",
      "Opening link 157\n",
      "Opening link 158\n",
      "Opening link 159\n",
      "Opening link 160\n",
      "Opening link 161\n",
      "Opening link 162\n",
      "Opening link 163\n",
      "Opening link 164\n",
      "Opening link 165\n",
      "Opening link 166\n",
      "Opening link 167\n",
      "Opening link 168\n",
      "Opening link 169\n",
      "Opening link 170\n",
      "Opening link 171\n",
      "Opening link 172\n",
      "Opening link 173\n",
      "Opening link 174\n",
      "Opening link 175\n",
      "Opening link 176\n",
      "Opening link 177\n",
      "Opening link 178\n",
      "Opening link 179\n",
      "Opening link 180\n",
      "Opening link 181\n",
      "Opening link 182\n",
      "Opening link 183\n",
      "Opening link 184\n",
      "Opening link 185\n",
      "Opening link 186\n",
      "Opening link 187\n",
      "Opening link 188\n",
      "Opening link 189\n",
      "Opening link 190\n",
      "Opening link 191\n",
      "Opening link 192\n",
      "Opening link 193\n",
      "Opening link 194\n",
      "Opening link 195\n",
      "Opening link 196\n",
      "Opening link 197\n",
      "Opening link 198\n",
      "Opening link 199\n",
      "Opening link 200\n",
      "Opening link 201\n",
      "Opening link 202\n",
      "Opening link 203\n",
      "Opening link 204\n",
      "Opening link 205\n",
      "Opening link 206\n",
      "Opening link 207\n",
      "Opening link 208\n",
      "Opening link 209\n",
      "Opening link 210\n",
      "Opening link 211\n",
      "Opening link 212\n",
      "Opening link 213\n",
      "Opening link 214\n",
      "Opening link 215\n",
      "Opening link 216\n",
      "Opening link 217\n",
      "Opening link 218\n",
      "Opening link 219\n",
      "Opening link 220\n",
      "Opening link 221\n",
      "Opening link 222\n",
      "Opening link 223\n",
      "Opening link 224\n",
      "Opening link 225\n",
      "Opening link 226\n",
      "Opening link 227\n",
      "Opening link 228\n",
      "Opening link 229\n",
      "Opening link 230\n",
      "Opening link 231\n",
      "Opening link 232\n",
      "Opening link 233\n",
      "Opening link 234\n",
      "Opening link 235\n",
      "Opening link 236\n",
      "Opening link 237\n",
      "Opening link 238\n",
      "Opening link 239\n",
      "Opening link 240\n",
      "Opening link 241\n",
      "Opening link 242\n",
      "Opening link 243\n",
      "Opening link 244\n",
      "Opening link 245\n",
      "Opening link 246\n",
      "Opening link 247\n",
      "Opening link 248\n",
      "Opening link 249\n",
      "Opening link 250\n",
      "Opening link 251\n",
      "Opening link 252\n",
      "Opening link 253\n",
      "Opening link 254\n",
      "Opening link 255\n",
      "Opening link 256\n",
      "Opening link 257\n",
      "Opening link 258\n",
      "Opening link 259\n",
      "Opening link 260\n",
      "Opening link 261\n",
      "Opening link 262\n",
      "Opening link 263\n",
      "Opening link 264\n",
      "Opening link 265\n",
      "Opening link 266\n",
      "Opening link 267\n",
      "Opening link 268\n",
      "Opening link 269\n",
      "Opening link 270\n",
      "Opening link 271\n",
      "Opening link 272\n",
      "Opening link 273\n",
      "Opening link 274\n",
      "Opening link 275\n",
      "Opening link 276\n",
      "Opening link 277\n",
      "Opening link 278\n",
      "Opening link 279\n",
      "Opening link 280\n",
      "Opening link 281\n",
      "Opening link 282\n",
      "Opening link 283\n",
      "Opening link 284\n",
      "Opening link 285\n",
      "Opening link 286\n",
      "Opening link 287\n",
      "Opening link 288\n",
      "Opening link 289\n",
      "Opening link 290\n",
      "Opening link 291\n",
      "Opening link 292\n",
      "Opening link 293\n",
      "Opening link 294\n",
      "Opening link 295\n",
      "Opening link 296\n",
      "Opening link 297\n",
      "Opening link 298\n",
      "Opening link 299\n",
      "Opening link 300\n",
      "Opening link 301\n",
      "Opening link 302\n",
      "Opening link 303\n",
      "Opening link 304\n",
      "Opening link 305\n",
      "Opening link 306\n",
      "Opening link 307\n",
      "Opening link 308\n",
      "Opening link 309\n",
      "Opening link 310\n",
      "Opening link 311\n",
      "Opening link 312\n",
      "Opening link 313\n",
      "Opening link 314\n",
      "Opening link 315\n",
      "Opening link 316\n",
      "Opening link 317\n",
      "Opening link 318\n",
      "Opening link 319\n",
      "Opening link 320\n",
      "Opening link 321\n",
      "Opening link 322\n",
      "Opening link 323\n",
      "Opening link 324\n",
      "Opening link 325\n",
      "Opening link 326\n",
      "Opening link 327\n",
      "Opening link 328\n",
      "Opening link 329\n",
      "Opening link 330\n",
      "Opening link 331\n",
      "Opening link 332\n",
      "Opening link 333\n",
      "Opening link 334\n",
      "Opening link 335\n",
      "Opening link 336\n",
      "Opening link 337\n",
      "Opening link 338\n",
      "Opening link 339\n",
      "Opening link 340\n",
      "Opening link 341\n",
      "Opening link 342\n",
      "Opening link 343\n",
      "Opening link 344\n",
      "Opening link 345\n",
      "Opening link 346\n",
      "Opening link 347\n",
      "Opening link 348\n",
      "Opening link 349\n",
      "Opening link 350\n",
      "Opening link 351\n",
      "Opening link 352\n",
      "Opening link 353\n",
      "Opening link 354\n",
      "Opening link 355\n",
      "Opening link 356\n",
      "Opening link 357\n",
      "Opening link 358\n",
      "Opening link 359\n",
      "Opening link 360\n",
      "Opening link 361\n",
      "Opening link 362\n",
      "Opening link 363\n",
      "Opening link 364\n",
      "Opening link 365\n",
      "Opening link 366\n",
      "Opening link 367\n",
      "Opening link 368\n",
      "Opening link 369\n",
      "Opening link 370\n",
      "Opening link 371\n",
      "Opening link 372\n",
      "Opening link 373\n",
      "Opening link 374\n",
      "Opening link 375\n",
      "Opening link 376\n",
      "Opening link 377\n",
      "Opening link 378\n",
      "Opening link 379\n",
      "Opening link 380\n",
      "Opening link 381\n",
      "Opening link 382\n",
      "Opening link 383\n",
      "Opening link 384\n",
      "Opening link 385\n",
      "Opening link 386\n",
      "Opening link 387\n",
      "Opening link 388\n",
      "Opening link 389\n",
      "Opening link 390\n",
      "Opening link 391\n",
      "Opening link 392\n",
      "Opening link 393\n",
      "Opening link 394\n",
      "Opening link 395\n",
      "Opening link 396\n",
      "Opening link 397\n",
      "Opening link 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f8501abda60>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dragon/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m author_text \u001b[38;5;241m=\u001b[39m button\u001b[38;5;241m.\u001b[39mtext  \u001b[38;5;66;03m# Store the text on the button as the author\u001b[39;00m\n\u001b[1;32m     45\u001b[0m button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m---> 46\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pause to allow the side panel to open\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Locate the side panel content\u001b[39;00m\n\u001b[1;32m     49\u001b[0m side_panel_content \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mside-panel-content\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"data_final.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Function to initialize the WebDriver in headless mode\n",
    "def initialize_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Enforce headless mode\n",
    "    options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration if present\n",
    "    options.add_argument(\"--no-sandbox\")  # Sandbox may cause issues, disable it\n",
    "    service = Service(executable_path=\"/home/dragon/Git/pythonclass/webscrapping school/geckodriver\")\n",
    "    driver = webdriver.Firefox(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "# Lists to store the data\n",
    "authors = []\n",
    "affiliations = []\n",
    "links = []\n",
    "\n",
    "# Iterate through all links in the 'Link' column\n",
    "for i, link in enumerate(df['Link']):\n",
    "    try:\n",
    "        # Initialize the WebDriver for each link\n",
    "        driver = initialize_driver()\n",
    "        print(f\"Opening link {i + 1}\")  # Print the current link number\n",
    "        driver.get(link)\n",
    "        time.sleep(5)  # Pause for 5 seconds to let the page load fully\n",
    "\n",
    "        # Locate the div with id=\"author-group\"\n",
    "        author_group_div = driver.find_element(By.ID, \"author-group\")\n",
    "\n",
    "        # Find all buttons within the div with the specified classes\n",
    "        buttons = author_group_div.find_elements(By.CSS_SELECTOR, \".button-link.button-link-secondary.button-link-underline\")\n",
    "        \n",
    "        # Click each button\n",
    "        for button in buttons:\n",
    "            author_text = button.text  # Store the text on the button as the author\n",
    "            button.click()\n",
    "            time.sleep(3)  # Pause to allow the side panel to open\n",
    "            \n",
    "            # Locate the side panel content\n",
    "            side_panel_content = driver.find_element(By.CLASS_NAME, \"side-panel-content\")\n",
    "            \n",
    "            # Locate the div with class=\"affiliation\" within the side panel\n",
    "            affiliation_div = side_panel_content.find_element(By.CLASS_NAME, \"affiliation\")\n",
    "            affiliation_text = affiliation_div.text  # Collect the text as the affiliation\n",
    "            \n",
    "            # Store the link, author, and affiliation\n",
    "            links.append(link)\n",
    "            authors.append(author_text)\n",
    "            affiliations.append(affiliation_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing link {i + 1}: {e}\")\n",
    "        links.append(link)\n",
    "        authors.append(\"N/A\")\n",
    "        affiliations.append(\"N/A\")\n",
    "    \n",
    "    finally:\n",
    "        # Terminate the WebDriver after processing the link\n",
    "        driver.quit()\n",
    "\n",
    "# Combine the collected data into a DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'Link': links,\n",
    "    'Author': authors,\n",
    "    'Affiliation': affiliations\n",
    "})\n",
    "\n",
    "# Save the output to an Excel file or CSV\n",
    "output_file_path = \"collected_data.xlsx\"\n",
    "output_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Data collected and saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected and saved to collected_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine the collected data into a DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'Link': links,\n",
    "    'Author': authors,\n",
    "    'Affiliation': affiliations\n",
    "})\n",
    "\n",
    "# Save the output to an Excel file or CSV\n",
    "output_file_path = \"collected_data.xlsx\"\n",
    "output_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Data collected and saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link 1 already scraped, skipping.\n",
      "Link 2 already scraped, skipping.\n",
      "Link 3 already scraped, skipping.\n",
      "Link 4 already scraped, skipping.\n",
      "Link 5 already scraped, skipping.\n",
      "Link 6 already scraped, skipping.\n",
      "Link 7 already scraped, skipping.\n",
      "Link 8 already scraped, skipping.\n",
      "Link 9 already scraped, skipping.\n",
      "Link 10 already scraped, skipping.\n",
      "Link 11 already scraped, skipping.\n",
      "Link 12 already scraped, skipping.\n",
      "Link 13 already scraped, skipping.\n",
      "Link 14 already scraped, skipping.\n",
      "Link 15 already scraped, skipping.\n",
      "Link 16 already scraped, skipping.\n",
      "Link 17 already scraped, skipping.\n",
      "Link 18 already scraped, skipping.\n",
      "Link 19 already scraped, skipping.\n",
      "Link 20 already scraped, skipping.\n",
      "Link 21 already scraped, skipping.\n",
      "Link 22 already scraped, skipping.\n",
      "Link 23 already scraped, skipping.\n",
      "Link 24 already scraped, skipping.\n",
      "Link 25 already scraped, skipping.\n",
      "Link 26 already scraped, skipping.\n",
      "Link 27 already scraped, skipping.\n",
      "Link 28 already scraped, skipping.\n",
      "Link 29 already scraped, skipping.\n",
      "Link 30 already scraped, skipping.\n",
      "Link 31 already scraped, skipping.\n",
      "Link 32 already scraped, skipping.\n",
      "Link 33 already scraped, skipping.\n",
      "Link 34 already scraped, skipping.\n",
      "Link 35 already scraped, skipping.\n",
      "Link 36 already scraped, skipping.\n",
      "Link 37 already scraped, skipping.\n",
      "Link 38 already scraped, skipping.\n",
      "Link 39 already scraped, skipping.\n",
      "Link 40 already scraped, skipping.\n",
      "Link 41 already scraped, skipping.\n",
      "Link 42 already scraped, skipping.\n",
      "Link 43 already scraped, skipping.\n",
      "Link 44 already scraped, skipping.\n",
      "Link 45 already scraped, skipping.\n",
      "Link 46 already scraped, skipping.\n",
      "Link 47 already scraped, skipping.\n",
      "Link 48 already scraped, skipping.\n",
      "Link 49 already scraped, skipping.\n",
      "Link 50 already scraped, skipping.\n",
      "Link 51 already scraped, skipping.\n",
      "Link 52 already scraped, skipping.\n",
      "Link 53 already scraped, skipping.\n",
      "Link 54 already scraped, skipping.\n",
      "Link 55 already scraped, skipping.\n",
      "Link 56 already scraped, skipping.\n",
      "Link 57 already scraped, skipping.\n",
      "Link 58 already scraped, skipping.\n",
      "Link 59 already scraped, skipping.\n",
      "Link 60 already scraped, skipping.\n",
      "Link 61 already scraped, skipping.\n",
      "Link 62 already scraped, skipping.\n",
      "Link 63 already scraped, skipping.\n",
      "Link 64 already scraped, skipping.\n",
      "Link 65 already scraped, skipping.\n",
      "Link 66 already scraped, skipping.\n",
      "Link 67 already scraped, skipping.\n",
      "Link 68 already scraped, skipping.\n",
      "Link 69 already scraped, skipping.\n",
      "Link 70 already scraped, skipping.\n",
      "Link 71 already scraped, skipping.\n",
      "Link 72 already scraped, skipping.\n",
      "Link 73 already scraped, skipping.\n",
      "Link 74 already scraped, skipping.\n",
      "Link 75 already scraped, skipping.\n",
      "Link 76 already scraped, skipping.\n",
      "Link 77 already scraped, skipping.\n",
      "Link 78 already scraped, skipping.\n",
      "Link 79 already scraped, skipping.\n",
      "Link 80 already scraped, skipping.\n",
      "Link 81 already scraped, skipping.\n",
      "Link 82 already scraped, skipping.\n",
      "Link 83 already scraped, skipping.\n",
      "Link 84 already scraped, skipping.\n",
      "Link 85 already scraped, skipping.\n",
      "Link 86 already scraped, skipping.\n",
      "Link 87 already scraped, skipping.\n",
      "Link 88 already scraped, skipping.\n",
      "Link 89 already scraped, skipping.\n",
      "Link 90 already scraped, skipping.\n",
      "Link 91 already scraped, skipping.\n",
      "Link 92 already scraped, skipping.\n",
      "Link 93 already scraped, skipping.\n",
      "Link 94 already scraped, skipping.\n",
      "Link 95 already scraped, skipping.\n",
      "Link 96 already scraped, skipping.\n",
      "Link 97 already scraped, skipping.\n",
      "Link 98 already scraped, skipping.\n",
      "Link 99 already scraped, skipping.\n",
      "Link 100 already scraped, skipping.\n",
      "Link 101 already scraped, skipping.\n",
      "Link 102 already scraped, skipping.\n",
      "Link 103 already scraped, skipping.\n",
      "Link 104 already scraped, skipping.\n",
      "Link 105 already scraped, skipping.\n",
      "Link 106 already scraped, skipping.\n",
      "Link 107 already scraped, skipping.\n",
      "Link 108 already scraped, skipping.\n",
      "Link 109 already scraped, skipping.\n",
      "Link 110 already scraped, skipping.\n",
      "Link 111 already scraped, skipping.\n",
      "Link 112 already scraped, skipping.\n",
      "Link 113 already scraped, skipping.\n",
      "Link 114 already scraped, skipping.\n",
      "Link 115 already scraped, skipping.\n",
      "Link 116 already scraped, skipping.\n",
      "Link 117 already scraped, skipping.\n",
      "Link 118 already scraped, skipping.\n",
      "Link 119 already scraped, skipping.\n",
      "Link 120 already scraped, skipping.\n",
      "Link 121 already scraped, skipping.\n",
      "Link 122 already scraped, skipping.\n",
      "Link 123 already scraped, skipping.\n",
      "Link 124 already scraped, skipping.\n",
      "Link 125 already scraped, skipping.\n",
      "Link 126 already scraped, skipping.\n",
      "Link 127 already scraped, skipping.\n",
      "Link 128 already scraped, skipping.\n",
      "Link 129 already scraped, skipping.\n",
      "Link 130 already scraped, skipping.\n",
      "Link 131 already scraped, skipping.\n",
      "Link 132 already scraped, skipping.\n",
      "Link 133 already scraped, skipping.\n",
      "Link 134 already scraped, skipping.\n",
      "Link 135 already scraped, skipping.\n",
      "Link 136 already scraped, skipping.\n",
      "Link 137 already scraped, skipping.\n",
      "Link 138 already scraped, skipping.\n",
      "Link 139 already scraped, skipping.\n",
      "Link 140 already scraped, skipping.\n",
      "Link 141 already scraped, skipping.\n",
      "Link 142 already scraped, skipping.\n",
      "Link 143 already scraped, skipping.\n",
      "Link 144 already scraped, skipping.\n",
      "Link 145 already scraped, skipping.\n",
      "Link 146 already scraped, skipping.\n",
      "Link 147 already scraped, skipping.\n",
      "Link 148 already scraped, skipping.\n",
      "Link 149 already scraped, skipping.\n",
      "Link 150 already scraped, skipping.\n",
      "Link 151 already scraped, skipping.\n",
      "Link 152 already scraped, skipping.\n",
      "Link 153 already scraped, skipping.\n",
      "Link 154 already scraped, skipping.\n",
      "Link 155 already scraped, skipping.\n",
      "Link 156 already scraped, skipping.\n",
      "Link 157 already scraped, skipping.\n",
      "Link 158 already scraped, skipping.\n",
      "Link 159 already scraped, skipping.\n",
      "Link 160 already scraped, skipping.\n",
      "Link 161 already scraped, skipping.\n",
      "Link 162 already scraped, skipping.\n",
      "Link 163 already scraped, skipping.\n",
      "Link 164 already scraped, skipping.\n",
      "Link 165 already scraped, skipping.\n",
      "Link 166 already scraped, skipping.\n",
      "Link 167 already scraped, skipping.\n",
      "Link 168 already scraped, skipping.\n",
      "Link 169 already scraped, skipping.\n",
      "Link 170 already scraped, skipping.\n",
      "Link 171 already scraped, skipping.\n",
      "Link 172 already scraped, skipping.\n",
      "Link 173 already scraped, skipping.\n",
      "Link 174 already scraped, skipping.\n",
      "Link 175 already scraped, skipping.\n",
      "Link 176 already scraped, skipping.\n",
      "Link 177 already scraped, skipping.\n",
      "Link 178 already scraped, skipping.\n",
      "Link 179 already scraped, skipping.\n",
      "Link 180 already scraped, skipping.\n",
      "Link 181 already scraped, skipping.\n",
      "Link 182 already scraped, skipping.\n",
      "Link 183 already scraped, skipping.\n",
      "Link 184 already scraped, skipping.\n",
      "Link 185 already scraped, skipping.\n",
      "Link 186 already scraped, skipping.\n",
      "Link 187 already scraped, skipping.\n",
      "Link 188 already scraped, skipping.\n",
      "Link 189 already scraped, skipping.\n",
      "Link 190 already scraped, skipping.\n",
      "Link 191 already scraped, skipping.\n",
      "Link 192 already scraped, skipping.\n",
      "Link 193 already scraped, skipping.\n",
      "Link 194 already scraped, skipping.\n",
      "Link 195 already scraped, skipping.\n",
      "Link 196 already scraped, skipping.\n",
      "Link 197 already scraped, skipping.\n",
      "Link 198 already scraped, skipping.\n",
      "Link 199 already scraped, skipping.\n",
      "Link 200 already scraped, skipping.\n",
      "Link 201 already scraped, skipping.\n",
      "Link 202 already scraped, skipping.\n",
      "Link 203 already scraped, skipping.\n",
      "Link 204 already scraped, skipping.\n",
      "Link 205 already scraped, skipping.\n",
      "Link 206 already scraped, skipping.\n",
      "Link 207 already scraped, skipping.\n",
      "Link 208 already scraped, skipping.\n",
      "Link 209 already scraped, skipping.\n",
      "Link 210 already scraped, skipping.\n",
      "Link 211 already scraped, skipping.\n",
      "Link 212 already scraped, skipping.\n",
      "Link 213 already scraped, skipping.\n",
      "Link 214 already scraped, skipping.\n",
      "Link 215 already scraped, skipping.\n",
      "Link 216 already scraped, skipping.\n",
      "Link 217 already scraped, skipping.\n",
      "Link 218 already scraped, skipping.\n",
      "Link 219 already scraped, skipping.\n",
      "Link 220 already scraped, skipping.\n",
      "Link 221 already scraped, skipping.\n",
      "Link 222 already scraped, skipping.\n",
      "Link 223 already scraped, skipping.\n",
      "Link 224 already scraped, skipping.\n",
      "Link 225 already scraped, skipping.\n",
      "Link 226 already scraped, skipping.\n",
      "Link 227 already scraped, skipping.\n",
      "Link 228 already scraped, skipping.\n",
      "Link 229 already scraped, skipping.\n",
      "Link 230 already scraped, skipping.\n",
      "Link 231 already scraped, skipping.\n",
      "Link 232 already scraped, skipping.\n",
      "Link 233 already scraped, skipping.\n",
      "Link 234 already scraped, skipping.\n",
      "Link 235 already scraped, skipping.\n",
      "Link 236 already scraped, skipping.\n",
      "Link 237 already scraped, skipping.\n",
      "Link 238 already scraped, skipping.\n",
      "Link 239 already scraped, skipping.\n",
      "Link 240 already scraped, skipping.\n",
      "Link 241 already scraped, skipping.\n",
      "Link 242 already scraped, skipping.\n",
      "Link 243 already scraped, skipping.\n",
      "Link 244 already scraped, skipping.\n",
      "Link 245 already scraped, skipping.\n",
      "Link 246 already scraped, skipping.\n",
      "Link 247 already scraped, skipping.\n",
      "Link 248 already scraped, skipping.\n",
      "Link 249 already scraped, skipping.\n",
      "Link 250 already scraped, skipping.\n",
      "Link 251 already scraped, skipping.\n",
      "Link 252 already scraped, skipping.\n",
      "Link 253 already scraped, skipping.\n",
      "Link 254 already scraped, skipping.\n",
      "Link 255 already scraped, skipping.\n",
      "Link 256 already scraped, skipping.\n",
      "Link 257 already scraped, skipping.\n",
      "Link 258 already scraped, skipping.\n",
      "Link 259 already scraped, skipping.\n",
      "Link 260 already scraped, skipping.\n",
      "Link 261 already scraped, skipping.\n",
      "Link 262 already scraped, skipping.\n",
      "Link 263 already scraped, skipping.\n",
      "Link 264 already scraped, skipping.\n",
      "Link 265 already scraped, skipping.\n",
      "Link 266 already scraped, skipping.\n",
      "Link 267 already scraped, skipping.\n",
      "Link 268 already scraped, skipping.\n",
      "Link 269 already scraped, skipping.\n",
      "Link 270 already scraped, skipping.\n",
      "Link 271 already scraped, skipping.\n",
      "Link 272 already scraped, skipping.\n",
      "Link 273 already scraped, skipping.\n",
      "Link 274 already scraped, skipping.\n",
      "Link 275 already scraped, skipping.\n",
      "Link 276 already scraped, skipping.\n",
      "Link 277 already scraped, skipping.\n",
      "Link 278 already scraped, skipping.\n",
      "Link 279 already scraped, skipping.\n",
      "Link 280 already scraped, skipping.\n",
      "Link 281 already scraped, skipping.\n",
      "Link 282 already scraped, skipping.\n",
      "Link 283 already scraped, skipping.\n",
      "Link 284 already scraped, skipping.\n",
      "Link 285 already scraped, skipping.\n",
      "Link 286 already scraped, skipping.\n",
      "Link 287 already scraped, skipping.\n",
      "Link 288 already scraped, skipping.\n",
      "Link 289 already scraped, skipping.\n",
      "Link 290 already scraped, skipping.\n",
      "Link 291 already scraped, skipping.\n",
      "Link 292 already scraped, skipping.\n",
      "Link 293 already scraped, skipping.\n",
      "Link 294 already scraped, skipping.\n",
      "Link 295 already scraped, skipping.\n",
      "Link 296 already scraped, skipping.\n",
      "Link 297 already scraped, skipping.\n",
      "Link 298 already scraped, skipping.\n",
      "Link 299 already scraped, skipping.\n",
      "Link 300 already scraped, skipping.\n",
      "Link 301 already scraped, skipping.\n",
      "Link 302 already scraped, skipping.\n",
      "Link 303 already scraped, skipping.\n",
      "Link 304 already scraped, skipping.\n",
      "Link 305 already scraped, skipping.\n",
      "Link 306 already scraped, skipping.\n",
      "Link 307 already scraped, skipping.\n",
      "Link 308 already scraped, skipping.\n",
      "Link 309 already scraped, skipping.\n",
      "Link 310 already scraped, skipping.\n",
      "Link 311 already scraped, skipping.\n",
      "Link 312 already scraped, skipping.\n",
      "Link 313 already scraped, skipping.\n",
      "Link 314 already scraped, skipping.\n",
      "Link 315 already scraped, skipping.\n",
      "Link 316 already scraped, skipping.\n",
      "Link 317 already scraped, skipping.\n",
      "Link 318 already scraped, skipping.\n",
      "Link 319 already scraped, skipping.\n",
      "Link 320 already scraped, skipping.\n",
      "Link 321 already scraped, skipping.\n",
      "Link 322 already scraped, skipping.\n",
      "Link 323 already scraped, skipping.\n",
      "Link 324 already scraped, skipping.\n",
      "Link 325 already scraped, skipping.\n",
      "Link 326 already scraped, skipping.\n",
      "Link 327 already scraped, skipping.\n",
      "Link 328 already scraped, skipping.\n",
      "Link 329 already scraped, skipping.\n",
      "Link 330 already scraped, skipping.\n",
      "Link 331 already scraped, skipping.\n",
      "Link 332 already scraped, skipping.\n",
      "Link 333 already scraped, skipping.\n",
      "Link 334 already scraped, skipping.\n",
      "Link 335 already scraped, skipping.\n",
      "Link 336 already scraped, skipping.\n",
      "Link 337 already scraped, skipping.\n",
      "Link 338 already scraped, skipping.\n",
      "Link 339 already scraped, skipping.\n",
      "Link 340 already scraped, skipping.\n",
      "Link 341 already scraped, skipping.\n",
      "Link 342 already scraped, skipping.\n",
      "Link 343 already scraped, skipping.\n",
      "Link 344 already scraped, skipping.\n",
      "Link 345 already scraped, skipping.\n",
      "Link 346 already scraped, skipping.\n",
      "Link 347 already scraped, skipping.\n",
      "Link 348 already scraped, skipping.\n",
      "Link 349 already scraped, skipping.\n",
      "Link 350 already scraped, skipping.\n",
      "Link 351 already scraped, skipping.\n",
      "Link 352 already scraped, skipping.\n",
      "Link 353 already scraped, skipping.\n",
      "Link 354 already scraped, skipping.\n",
      "Link 355 already scraped, skipping.\n",
      "Link 356 already scraped, skipping.\n",
      "Link 357 already scraped, skipping.\n",
      "Link 358 already scraped, skipping.\n",
      "Link 359 already scraped, skipping.\n",
      "Link 360 already scraped, skipping.\n",
      "Link 361 already scraped, skipping.\n",
      "Link 362 already scraped, skipping.\n",
      "Link 363 already scraped, skipping.\n",
      "Link 364 already scraped, skipping.\n",
      "Link 365 already scraped, skipping.\n",
      "Link 366 already scraped, skipping.\n",
      "Link 367 already scraped, skipping.\n",
      "Link 368 already scraped, skipping.\n",
      "Link 369 already scraped, skipping.\n",
      "Link 370 already scraped, skipping.\n",
      "Link 371 already scraped, skipping.\n",
      "Link 372 already scraped, skipping.\n",
      "Link 373 already scraped, skipping.\n",
      "Link 374 already scraped, skipping.\n",
      "Link 375 already scraped, skipping.\n",
      "Link 376 already scraped, skipping.\n",
      "Link 377 already scraped, skipping.\n",
      "Link 378 already scraped, skipping.\n",
      "Link 379 already scraped, skipping.\n",
      "Link 380 already scraped, skipping.\n",
      "Link 381 already scraped, skipping.\n",
      "Link 382 already scraped, skipping.\n",
      "Link 383 already scraped, skipping.\n",
      "Link 384 already scraped, skipping.\n",
      "Link 385 already scraped, skipping.\n",
      "Link 386 already scraped, skipping.\n",
      "Link 387 already scraped, skipping.\n",
      "Link 388 already scraped, skipping.\n",
      "Link 389 already scraped, skipping.\n",
      "Link 390 already scraped, skipping.\n",
      "Link 391 already scraped, skipping.\n",
      "Link 392 already scraped, skipping.\n",
      "Link 393 already scraped, skipping.\n",
      "Link 394 already scraped, skipping.\n",
      "Link 395 already scraped, skipping.\n",
      "Link 396 already scraped, skipping.\n",
      "Link 397 already scraped, skipping.\n",
      "Link 398 already scraped, skipping.\n",
      "Link 399 already scraped, skipping.\n",
      "Link 400 already scraped, skipping.\n",
      "Opening link 401: https://www.sciencedirect.com//science/article/pii/S2212827117301233\n",
      "Data for author 'Stefan Junk a' added.\n",
      "Data for author 'Werner Schröder b' added.\n",
      "Data for author 'Steffen Schrock a' added.\n",
      "Opening link 402: https://www.sciencedirect.com//science/article/pii/S1871678416324980\n",
      "Data for author 'Felix Krujatz a' added.\n",
      "Data for author 'Anja Lode b' added.\n",
      "Data for author 'Julia Seidel a' added.\n",
      "Data for author 'Thomas Bley a' added.\n",
      "Data for author 'Michael Gelinsky b' added.\n",
      "Data for author 'Juliane Steingroewer a' added.\n",
      "Opening link 403: https://www.sciencedirect.com//science/article/pii/S221282711730464X\n",
      "Data for author 'Florian Baumann M.Sc. a' added.\n",
      "Data for author 'Julian Scholz M.Sc. b' added.\n",
      "Data for author 'Jürgen Fleischer Prof. Dr.-Ing. a' added.\n",
      "Opening link 404: https://www.sciencedirect.com//science/article/pii/S1877705817358411\n",
      "Data for author 'Irina Sizova a' added.\n",
      "Data for author 'Markus Bambach a' added.\n",
      "Opening link 405: https://www.sciencedirect.com//science/article/pii/S2212827117306443\n",
      "Data for author 'Tobias Kamps a' added.\n",
      "Data for author 'Melanie Gralow a' added.\n",
      "Data for author 'Georg Schlick a' added.\n",
      "Data for author 'Gunther Reinhart a b' added.\n",
      "Opening link 406: https://www.sciencedirect.com//science/article/pii/S2405844017306680\n",
      "Data for author 'A-F. Obaton a' added.\n",
      "Data for author 'J. Fain b' added.\n",
      "Data for author 'M. Djemaï b' added.\n",
      "Data for author 'D. Meinel c' added.\n",
      "Data for author 'F. Léonard c' added.\n",
      "Data for author 'E. Mahé a' added.\n",
      "Data for author 'B. Lécuelle d' added.\n",
      "Data for author 'J-J. Fouchet b' added.\n",
      "Data for author 'G. Bruno c' added.\n",
      "Opening link 407: https://www.sciencedirect.com//science/article/pii/S0928493117308214\n",
      "Data for author 'Jodie N. Haigh a b' added.\n",
      "Data for author 'Tim R. Dargaville a' added.\n",
      "Data for author 'Paul D. Dalton b' added.\n",
      "Opening link 408: https://www.sciencedirect.com//science/article/pii/B9780081004333000117\n",
      "Data for author 'C.F.W. Lindemann' added.\n",
      "Data for author 'U. Jahnke' added.\n",
      "Opening link 409: https://www.sciencedirect.com//science/article/pii/S2452321617304122\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpening link \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlink\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print the current link number\u001b[39;00m\n\u001b[1;32m     42\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(link)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pause for 5 seconds to let the page load fully\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Locate the div with id=\"author-group\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m author_group_div \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor-group\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Load the Excel file with links to be scraped\n",
    "file_path = \"data_final.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Load the Excel file with already collected data\n",
    "collected_file_path = \"collected_data.xlsx\"\n",
    "try:\n",
    "    collected_df = pd.read_excel(collected_file_path)\n",
    "    existing_links = set(collected_df['Link'])  # Convert to set for faster lookup\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create an empty DataFrame and set\n",
    "    collected_df = pd.DataFrame(columns=['Link', 'Author', 'Affiliation'])\n",
    "    existing_links = set()\n",
    "\n",
    "# Function to initialize the WebDriver in headless mode\n",
    "def initialize_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Enforce headless mode\n",
    "    options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration if present\n",
    "    options.add_argument(\"--no-sandbox\")  # Sandbox may cause issues, disable it\n",
    "    service = Service(executable_path=\"/home/dragon/Git/pythonclass/webscrapping school/geckodriver\")\n",
    "    driver = webdriver.Firefox(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "# Iterate through all links in the 'Link' column of the initial Excel file\n",
    "for i, link in enumerate(df['Link']):\n",
    "    if link in existing_links:\n",
    "        print(f\"Link {i + 1} already scraped, skipping.\")\n",
    "        continue  # Skip already scraped links\n",
    "    \n",
    "    try:\n",
    "        # Initialize the WebDriver\n",
    "        driver = initialize_driver()\n",
    "        print(f\"Opening link {i + 1}: {link}\")  # Print the current link number\n",
    "        driver.get(link)\n",
    "        time.sleep(5)  # Pause for 5 seconds to let the page load fully\n",
    "\n",
    "        # Locate the div with id=\"author-group\"\n",
    "        author_group_div = driver.find_element(By.ID, \"author-group\")\n",
    "\n",
    "        # Find all buttons within the div with the specified classes\n",
    "        buttons = author_group_div.find_elements(By.CSS_SELECTOR, \".button-link.button-link-secondary.button-link-underline\")\n",
    "        \n",
    "        # Collect authors and their affiliations\n",
    "        for button in buttons:\n",
    "            author_text = button.text  # Store the text on the button as the author\n",
    "            button.click()\n",
    "            time.sleep(3)  # Pause to allow the side panel to open\n",
    "            \n",
    "            # Locate the side panel content\n",
    "            side_panel_content = driver.find_element(By.CLASS_NAME, \"side-panel-content\")\n",
    "            \n",
    "            # Locate the div with class=\"affiliation\" within the side panel\n",
    "            affiliation_div = side_panel_content.find_element(By.CLASS_NAME, \"affiliation\")\n",
    "            affiliation_text = affiliation_div.text  # Collect the text as the affiliation\n",
    "            \n",
    "            # Append the new data to the collected DataFrame\n",
    "            new_data = pd.DataFrame({'Link': [link], 'Author': [author_text], 'Affiliation': [affiliation_text]})\n",
    "            collected_df = pd.concat([collected_df, new_data], ignore_index=True)\n",
    "            \n",
    "            # Save the updated collected data to the Excel file\n",
    "            collected_df.to_excel(collected_file_path, index=False)\n",
    "            print(f\"Data for author '{author_text}' added.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing link {i + 1}: {e}\")\n",
    "        # Store N/A if there was an error, and save it to maintain the record\n",
    "        new_data = pd.DataFrame({'Link': [link], 'Author': [\"N/A\"], 'Affiliation': [\"N/A\"]})\n",
    "        collected_df = pd.concat([collected_df, new_data], ignore_index=True)\n",
    "        collected_df.to_excel(collected_file_path, index=False)\n",
    "    \n",
    "    finally:\n",
    "        # Terminate the WebDriver after processing the link\n",
    "        driver.quit()\n",
    "\n",
    "print(f\"Data collection completed. All data saved to {collected_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
